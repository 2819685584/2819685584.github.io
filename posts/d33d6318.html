<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MCP相关知识以及三种通信机制对比 | Notes  Blog</title><meta name="author" content="LHF"><meta name="copyright" content="LHF"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深入学习MCP相关知识，MCP三种通信机制（SSE、Stdio、StreamableHTTP）对比">
<meta property="og:type" content="article">
<meta property="og:title" content="MCP相关知识以及三种通信机制对比">
<meta property="og:url" content="https://www.liuhengfeng.xyz/posts/d33d6318.html">
<meta property="og:site_name" content="Notes  Blog">
<meta property="og:description" content="深入学习MCP相关知识，MCP三种通信机制（SSE、Stdio、StreamableHTTP）对比">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-06-06_00-40-46.jpg">
<meta property="article:published_time" content="2025-06-05T16:42:40.000Z">
<meta property="article:modified_time" content="2025-07-27T09:40:40.000Z">
<meta property="article:author" content="LHF">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-06-06_00-40-46.jpg"><link rel="shortcut icon" href="https://statics.liuhengfeng.xyz/Hexo/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240907191402.jpg"><link rel="canonical" href="https://www.liuhengfeng.xyz/posts/d33d6318.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MCP相关知识以及三种通信机制对比',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-27 17:40:40'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/custom.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/universe.css"><span id="fps"></span><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/hexo_electric_clock.css"><link rel="stylesheet" href="/css/bottom_runtime.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/lib/theme-chalk/index.css"><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/calendar.css"><style>.article-entry,.post-body,.post-content,#post,article,main,#content{-webkit-user-select:text!important;user-select:text!important}</style><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/u%3D1968668429%2C2104382916%26fm%3D253%26fmt%3Dauto%26app%3D138%26f%3DJPEG.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">79</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-06-06_00-40-46.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Notes  Blog"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/2876f803-ae9a-49ac-bcd9-9d623c5beb03.jpg"/><span class="site-name">Notes  Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MCP相关知识以及三种通信机制对比</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-05T16:42:40.000Z" title="发表于 2025-06-06 00:42:40">2025-06-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-27T09:40:40.000Z" title="更新于 2025-07-27 17:40:40">2025-07-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.8k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="MCP先导知识"><a href="#MCP先导知识" class="headerlink" title="MCP先导知识"></a>MCP先导知识</h1><p> 假设在PC本地用Python写了一个程序，这个程序可以爬取某个UP主的粉丝数量，只要把LLM和这个爬虫程序结合起来，把程序返回结果传递给LLM，LLM就能知道某UP主的粉丝数，相当于LLM突破了预训练知识库的知识边界限制，能更好地回答用户提出的预训练知识库之外的知识。</p>
<blockquote>
<p>如何让LLM运行用户本地程序呢?</p>
</blockquote>
<p>程序一般在用户本地PC上，LLM部署在模型提供商那里，跟本地程序不在一个局域网上，LLM无法直调用程序。<strong>但没有什么是加一层中间层解决不了的</strong>。可以再写一个程序（中介程序），通过API的方式来访问LLM。中介程序会把用户提出的问题连同本地工具库的使用说明书一起发送给LLM，LLM分析用户提出的问题之后，决定是否需要使用工具，如果需要调用工具，就会向中介程序返回调用信息，说明要调用某个工具以及调用时传入的参数，中介程序收到后，按照AI的要求调用本地或远端工具，并把工具调用结果发送给LLM，LLM分析工具输出的内容后，整合最终答复返回给了中介程序，用户能通过中介程序看到大模型回复问题的结果。</p>
<p><strong>中介程序可以是<code>Claude Desktop</code>、<code>Cherry Studio</code>、<code>Cursor</code>、<code>Cline</code>、<code>Dify</code>、<code>Coze</code>等，既有桌面客户端，也有IDE插件的方式，或者是Web方式。</strong></p>
<p><strong>可以把MCP协议类比为HTTP协议，MCP Host类比为各类浏览器，人主要与浏览器(相当于中介程序)打交道，MCP的原理与通过HTTP协议访问各种Server API相似。</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727151201483.png" alt="过程"></p>
<blockquote>
<p>工具使用说明书应该如何传递给大模型？</p>
</blockquote>
<p><strong>只要程序可交互，就一定有接口标准！！MCP只是制定了一个信息交换标准，信息交互标准由具体的MCP SDK实现，把最核心的Server端逻辑代码交给用户编写，其他的都交给MCP SDK内部实现</strong></p>
<p>最开始的时候，工具使用说明书直接写在用户提示词里，用户可以自定义对接方式，把工具名称、功能、参数、返回值都描述清楚即可。约定好如果要调用某个工具，返回的内容应该是什么样的，根据LLM返回的内容可以自己写程序进行对接，然后就能调用本地程序或远端程序接口了，但是早期的时候AI还不够聪明，遵循指令的效果并不好，无法按照提前约好的提示词内容返回准确的信息（智商不够，无法理解用户提示词里的自定义规范），所以无法达到很好地使用工具的效果。</p>
<hr>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727153448877.png" alt="OpenAI-Function calling规范"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 上述翻译</span><br><span class="line">您可以通过函数调用让模型访问您自己的自定义代码。根据系统提示和消息，模型可能会决定调用这些函数——而不是（或除了）生成文本或音频。然后，您将执行函数代码，发回结果，模型会将它们合并到最终响应中。</span><br></pre></td></tr></table></figure>

<p>后来，OpenAI在他们提供的ChatGPT模型里定义了规范，把工具传递的过程进行了标准化（<code>Function calling</code>），规范了工具使用说明书应该放在哪里、传递格式是什么、调用的返回格式是什么，把相关内容都约定好。<strong>实质上是独立使用JSON格式的交互信息来描述工具的相关信息，实现和用户输入提示词分离（不写在输入的提示词里）</strong>，以便AI看到JSON格式的信息时能清楚各个工具的用法；</p>
<blockquote>
<p>好好品鉴一下这段话</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">工具说明全放在提示词里是为了兼容市面上所有模型,其实openAI的function calling本质也是放在提示词里,只不过gpt的代码里给你封装了一层叫function calling的东西罢了,function calling也只是把指令按照约定的格式包装成提示词而已，只不过这个大模型可能针对这种格式做过调优，或者仅仅是系统提示词里有针对性的描述而已。一切大模型的调用最终都还是回到文本提示词。要记住,模型永远只能接受一段文本,是的,只有一段,无法并行读取,也无法分开,不管任何技术都是合并到一堆字符串里去,能不能完美理解出来全看LLM的智能程度,gpt刚出来的时候,我看到function calling就没去学了,因为这玩意只能GPT用,无法泛化到所有模型,现在的MCP封装了一层,弥补了function calling的短板,但如果你用的是ChatGPT api,那么MCP可以说是废的,多此一举,因为更消耗token,由于function calling是gpt源码里内置的,所以使用function calling的部分不计算token,更省钱,但现如今的技术肯定更看重泛化能力,谁都不想被一个LLM绑死,所以MCP是主流,但我认为这也不过是过渡性的产物罢了,海量的token消耗是个必须要解决的问题,如果MCP的源码在用GPT的时候,会自动转换成function calling来节省token,到后面LLM为了吸引用户,肯定会都内置function calling功能,这时MCP就变成了臃肿的老框架,因为它有很多适配的底层代码,这时候又会有一个新框架出现,主打效率、干净、整洁、无污染、低token,真香,计算机就是这样的,总是在封装,但封装过渡又无法很好向下兼容,所以又重开一个框架主打效率、简化,总是这么跳来跳去。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># function calling工具调用示例代码，要自己写一大堆工具schema，很麻烦</span><br><span class="line"></span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"># 要手动针对每一个工具做一大堆描述，工具越多，token消耗越多、context占用越多、需要手工写入的工具描述也越多</span><br><span class="line">tools = [&#123;</span><br><span class="line">    &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">    &quot;function&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_weather&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Get current temperature for a given location.&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;location&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                    &quot;description&quot;: &quot;City and country e.g. Bogotá, Colombia&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [</span><br><span class="line">                &quot;location&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;additionalProperties&quot;: False</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;strict&quot;: True</span><br><span class="line">    &#125;</span><br><span class="line">&#125;]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">    model=&quot;gpt-4.1&quot;,</span><br><span class="line">    messages=[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the weather like in Paris today?&quot;&#125;],</span><br><span class="line">    tools=tools</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(completion.choices[0].message.tool_calls)</span><br><span class="line"></span><br><span class="line"># LLM output——返回给本地的工具调用信息格式</span><br><span class="line">[&#123;</span><br><span class="line">    &quot;id&quot;: &quot;call_12345xyz&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">    &quot;function&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_weather&quot;,</span><br><span class="line">        &quot;arguments&quot;: &quot;&#123;\&quot;location\&quot;:\&quot;Paris, France\&quot;&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727154343172.png" alt="调用过程"></p>
<blockquote>
<p>从HTTP到MCP</p>
</blockquote>
<p>在<code>Coze</code>和<code>Dify</code>等低代码开发平台上，外部工具可以通过HTTP接口的方式暴露出来，只需要在平台上把外部工具的<code>API schema</code>配置好（API schema又是dify自定义的一种接口功能参数描述方法，可能会比function calling简单一点），Dify把<code>API schema</code>信息转换为标准的<code>Function Calling</code>工具说明书格式（套娃行为），从而实现让AI调用外部工具。在Dify上，每个外部工具都要编写专属的<code>API schema</code>，才能内置转换为LLM<code>Function Calling</code>格式的说明书，就像使用function calling需要为每一个tool配置json格式的说明一样，底层实现的是一对一的转换。一个AI智能体可能有上百个外部工具，每一个都需要编写<code>API schema</code>超级麻烦，如果后续接口有变化还要手动进行更新维护，费时费力。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727155859618.png" alt="API Schema转换"></p>
<p>这种操作过程太过麻烦，最好是在写MCP Server的时候就顺手把上面的事情做了（实际应该是MCP SDK内部实现的），只需要在开发MCP Server时写清注释和参数类型即可（标准注释类型，既包含函数功能说明，也包含参数和返回值说明）。此时外部工具不必由开发人员手动将HTTP API暴露出来，而是套上了一层MCP壳，通过这个壳统一对外提供服务。<code>Client</code>和<code>Server</code>之间使用的通信协议就是MCP协议，容纳<code>MCP Client</code>的中介程序就是<code>MCP Host</code>。</p>
<p>之后，可以使用MCP SDK快速开发外部工具，通过装饰器<code>@mcp.tool(&quot;func&quot;,description=&#39;xxx&#39;)</code>的方式非常简洁地实现工具函数，无需再编写web server来暴露函数（SSE和Streamable HTTP方式会自动暴露，stdio方式无需暴露），也无需编写繁琐的<code>API schema</code>文件来描述工具函数，里面的转换细节都由MCP SDK封装好了。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727155600840.png" alt="image-20250727155600840"></p>
<blockquote>
<p>编写MCP Server代码</p>
</blockquote>
<p>Server的工具函数备注内容非常重要，传统的备注是给程序员看的，但是MCP服务的备注是给大模型看的，它通过注释信息理解接口的功能和使用方法。在下面的<code>get_desktop_files()</code>方法上增加<code>@mcp.tool()</code>装饰器，是MCP SDK定义的规范，用于MCP的客户端自动发现MCP Server。</p>
<p>然后就可以在MCP Host中配置MCP Server了，配置方法参见各种工具，如Cursor、Cherry Studio、VSCode等。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from mcp.server.fastmcp import FastMCP</span><br><span class="line">mcp = FastMCP()</span><br><span class="line">@mcp.tool()</span><br><span class="line">def get_desktop_files():</span><br><span class="line">    &quot;&quot;&quot;获取桌面上的文件列表&quot;&quot;&quot;    </span><br><span class="line">    return os.listdir(os.path.expanduser(&quot;~/Desktop&quot;))</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    mcp.run(transport=&#x27;stdio&#x27;)	# 选择stdio通信方式运行</span><br><span class="line">    # mcp.run(transport=&#x27;sse&#x27;)	# 也可以选择sse通信方式，这种方式会默认启动一个Web Server</span><br></pre></td></tr></table></figure>

<h1 id="什么是MCP？"><a href="#什么是MCP？" class="headerlink" title="什么是MCP？"></a>什么是MCP？</h1><blockquote>
<p>回顾计算机发展史，TCP&#x2F;IP、HTTP等标准协议的建立曾催生互联网革命；在AI时代，MCP协议有望扮演类似的角色，成为智能应用互联互通的基石。</p>
</blockquote>
<p>现在的LLM模型具有强大的推理和泛化能力，但<strong>LLM本身受限于预训练时使用的训练集数据，无法实时获取最新的知识内容</strong>，LLM本身也无法使用外部工具获取外部数据源。<strong>MCP的目的就是为了解决LLM因数据孤岛限制而无法充分发挥本身潜力的问题，让LLM获得连接外部数据的能力。有了MCP，LLM能进一步走向智能体。</strong>通过MCP服务，大模型不再局限于只扮演大脑的角色，而是拥有了四肢，拥有了行动能力和获取数据的能力。</p>
<p>MCP（Model Context Protocol,模型上下文协议），是由 Anthropic在 2024 年 11 月推出并开源的一项创新标准。<strong>它是一种开放标准协议，用于将 AI 模型连接到各种外部工具和数据源</strong>，它定义了一套规则，告诉 AI 如何调用工具，包括参数名、参数类型和参数描述等信息，还能响应 AI 的 Toolcall 并返回结果，让 AI 携带结果继续发起对话；</p>
<p><strong>简单来说，MCP就像AI模型的”万能转接头”，让大模型能够以标准化的方式与外部数据源或工具进行交互，类似于TYPE-C能让不同设备能够通过相同的接口连接到PC。</strong></p>
<p>MCP官方文档：<a target="_blank" rel="noopener" href="https://modelcontextprotocol.io/overview">Model Context Protocol</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 官方描述</span><br><span class="line">Connect your AI applications to the world</span><br><span class="line">MCP is an open protocol that standardizes how applications provide context to large language models (LLMs). Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. MCP enables you build agents and complex workflows on top of LLMs and connects your models with the world.</span><br><span class="line"></span><br><span class="line"># Translate</span><br><span class="line">MCP 是一个开放协议，它标准化了应用程序向大型语言模型 (LLM) 提供上下文的方式。MCP 就像 AI 应用程序的 USB-C 端口一样。正如 USB-C 提供了一种标准化的方式将您的设备连接到各种外围设备和配件一样，MCP 也提供了一种标准化的方式将 AI 模型连接到不同的数据源和工具。MCP 使您能够在 LLM 之上构建agents和复杂的工作流，并将您的模型与世界连接起来。</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250606005652864.png" alt="MCP"></p>
<h2 id="MCP架构与工作流程"><a href="#MCP架构与工作流程" class="headerlink" title="MCP架构与工作流程"></a>MCP架构与工作流程</h2><blockquote>
<p>核心架构</p>
</blockquote>
<p>MCP遵循客户端-服务器架构（Client-Server，C—S架构），包含以下核心组件：</p>
<ul>
<li><p>MCP主机（MCP Hosts）</p>
<ul>
<li><p>发起API请求的LLM应用程序（Cursor、各种AI工具等）</p>
</li>
<li><p>负责接收用户输入的提示词输入并与LLM进行交互</p>
</li>
</ul>
</li>
<li><p>MCP客户端（MCP Clients）</p>
<ul>
<li><p>位于MCP Hosts内部，<strong>与MCP Server保持1:1的连接</strong></p>
</li>
<li><p>充当LLM和MCP Server之间的桥梁</p>
</li>
</ul>
</li>
<li><p>MCP服务器（MCP Servers）</p>
<ul>
<li><p>自定义开发的各种工具、接口</p>
</li>
<li><p>负责处理客户端发来的调用请求，根据传来的参数执行相应操作后返回执行结果</p>
</li>
</ul>
</li>
<li><p>资源（Resources）</p>
<ul>
<li><p>本地资源：本地计算机中可供MCP server安全访问的资源（如文件、数据库）</p>
</li>
<li><p>远程资源：MCP server可以连接的远程资源（如通过API访问服务）</p>
</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727164741410.png" alt="MCP架构"></p>
<blockquote>
<p>工作流程</p>
</blockquote>
<p>MCP的基本工作流程如下：</p>
<ul>
<li><p>初始化连接</p>
<ul>
<li>客户端向服务器发送连接请求，建立通信通道</li>
</ul>
</li>
<li><p>工具发现与选择</p>
<ul>
<li><p>MCP客户端从MCP服务器获取可用的工具列表</p>
</li>
<li><p><strong>将用户的问题连同工具描述一起发送给LLM</strong></p>
</li>
<li><p>LLM根据用户的问题决定是否需要使用工具以及使用哪些工具</p>
</li>
</ul>
</li>
<li><p>工具调用与执行</p>
<ul>
<li><p>如果需要使用工具，MCP客户端会通过MCP服务器执行相应的工具调用</p>
</li>
<li><p>MCP服务器处理请求，执行相应的操作（如查询数据库、读取文件等）</p>
</li>
</ul>
</li>
<li><p>结果处理与响应</p>
<ul>
<li><p>工具调用的结果会被发送回LLM</p>
</li>
<li><p>LLM基于所有信息生成自然语言响应</p>
</li>
<li><p>最后将响应展示给用户</p>
</li>
</ul>
</li>
<li><p>断开连接</p>
<ul>
<li>任务完成后，客户端可以主动关闭连接或等待服务器超时关闭</li>
</ul>
</li>
</ul>
<p><strong>简而言之：Client 首先与 Server 握手并获取工具清单，Host 把这些清单连同用法注入系统提示词；LLM 根据提示输出结构化调用（通常是 JSON）；Client 解析调用并向 Server 发起调用请求，收到结果后回传 Host，Host 再携带最新上下文进行下一轮模型推理。</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250606010717449.png" alt="MCP工作流程图"></p>
<h2 id="MCP应用场景"><a href="#MCP应用场景" class="headerlink" title="MCP应用场景"></a>MCP应用场景</h2><table>
<thead>
<tr>
<th>功能分类</th>
<th>功能项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>本地文件操作</td>
<td>文件读写</td>
<td>允许 AI 模型读取和创建本地文件</td>
</tr>
<tr>
<td></td>
<td>文档分析</td>
<td>分析本地文档并提取信息</td>
</tr>
<tr>
<td></td>
<td>代码管理</td>
<td>读取、分析和修改代码文件</td>
</tr>
<tr>
<td>数据库交互</td>
<td>数据查询</td>
<td>从本地或远程数据库获取信息</td>
</tr>
<tr>
<td></td>
<td>数据分析</td>
<td>分析数据库中的数据并生成报告</td>
</tr>
<tr>
<td></td>
<td>数据管理</td>
<td>更新、插入或删除数据库记录</td>
</tr>
<tr>
<td>API集成</td>
<td>网络服务调用</td>
<td>调用各种网络 API 获取信息</td>
</tr>
<tr>
<td></td>
<td>第三方服务集成</td>
<td>与第三方服务（如天气、股票、新闻等）集成</td>
</tr>
<tr>
<td></td>
<td>企业系统对接</td>
<td>与企业内部系统（如 CRM、ERP 等）对接</td>
</tr>
<tr>
<td>开发工具增强</td>
<td>IDE集成</td>
<td>增强代码编辑器和 IDE 的功能</td>
</tr>
<tr>
<td></td>
<td>调试辅助</td>
<td>帮助开发者调试代码</td>
</tr>
<tr>
<td>个人助理功能</td>
<td>日程管理</td>
<td>访问和管理日历</td>
</tr>
<tr>
<td></td>
<td>邮件处理</td>
<td>读取和撰写邮件</td>
</tr>
<tr>
<td></td>
<td>信息整理</td>
<td>整理和归类个人信息</td>
</tr>
</tbody></table>
<h1 id="JSON-RPC-2-0-协议"><a href="#JSON-RPC-2-0-协议" class="headerlink" title="JSON-RPC 2.0 协议"></a>JSON-RPC 2.0 协议</h1><p>MCP 中 Client 与 Server 使用 JSON-RPC 2.0 作为通信消息格式。JSON-RPC 是轻量级通信协议 一种轻量级的通信协议，其核心目标是屏蔽网络细节，使远程调用如同本地调用般简单，并可基于多种底层网络协议（如 TCP&#x2F;HTTP）实现。</p>
<p>JSON-RPC 的设计原则是简单性和互操作性，旨在为不同编程语言之间提供一种方便的方法来执行远程过程调用。通过定义一组简单的规则，任何能够解析 JSON 的语言或平台都可以实现 JSON-RPC 客户端或服务器。</p>
<p>JSON-RPC 2.0中只定义了两种消息，就是请求和响应，通知是一种特殊的请求，特点是不包含id字段。</p>
<blockquote>
<p>请求（Request）</p>
</blockquote>
<p>请求由<strong>MCP客户端或MCP服务器之一</strong>发给对方，调用对方的一个方法，并预期获得一个响应。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;jsonrpc&quot;: &quot;2.0&quot;,</span><br><span class="line">  &quot;method&quot;: &quot;方法名&quot;,</span><br><span class="line">  &quot;params&quot;: &#123;&quot;参数名&quot;: &quot;值&quot;&#125; | [&quot;值 1&quot;, &quot;值 2&quot;],  // 对象或数组</span><br><span class="line">  &quot;id&quot;: &quot;唯一ID&quot;                             // 可选（通知请求可省略）</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>响应（Response）</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;jsonrpc&quot;: &quot;2.0&quot;,</span><br><span class="line">  &quot;result&quot;: &quot;返回值&quot;,</span><br><span class="line">  &quot;id&quot;: &quot;对应请求 ID&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="MCP-与-Function-Calling-的关系"><a href="#MCP-与-Function-Calling-的关系" class="headerlink" title="MCP 与 Function Calling 的关系"></a>MCP 与 Function Calling 的关系</h1><p><code>Function Calling</code>是个非常不错的设计思路。但该技术有很大的不足——编写外部函数的工作量太大，一个简单的外部函数往往就得上百行代码（注释+参数说明+实现逻辑），而且为了让大模型认识这些函数，还需要额外为每个外部函数编写一个<code>JSON Schema</code>格式的功能说明(tools调用时放在tool列表里)。此外，还要为函数设计一个提示词模板<code>description</code>, 才能提高<code>Function Calling</code>响应的准确率。这种开发模式使得开发者要为每个系统单独编写适配接口，处理不同的认证方式、数据格式和调用规范，即使同样的功能，不同的人也会有N种开发版本，造成开发过程费时费力且不利于后期维护。<strong>简单来说，Function Calling其实就是用JSON重新描述了一个函数（接口）+注释</strong></p>
<p>MCP技术借助大模型Function Calling的基础能力，凭借高效的开发规范，被广为关注。<strong>MCP统一规范采用分布式架构</strong>，分为<strong>Client客户端</strong>和<strong>Server服务端</strong>两部分，客户端用户基于大模型编写(可以是.exe之类的可运行带GUI的程序，也可以是封装了LLM的py代码)，服务端暴露扩展大模型功能函数或接口。 <code>MCP</code>最突出的优势在于它的标准化程度：一次开发，多人复用。<strong>用户只需要编写一套客户端代码，就可以接入符合MCP开发规范的所有服务端（不管是自己编写的，还是利用别人编写的MCP Server）</strong>。这种统一的开发模式就像秦始皇的“书同文、车同轨”，有了统一的规范，只要本地运行的环境支持MCP协议，仅需几行代码就可以接入海量的外部MCP工具，提升智能体AI Agent的开发效率。Anthropic提供了一整套MCP客户端、服务端SDK等各种开发工具，并且支持Python， TypeScript和Java等多种语言，借助SDK, 仅需几行代码就可以快速开发一个MCP服务器。</p>
<p>例如，使用<code>Function Calling</code>技术开发天气查询功能要编写<code>description</code>, <code>json schema</code>前后大约150行代码，使用<code>MCP SDK</code>开发只需要编写一个请求函数<code>get_weather</code>并给出函数注释，借助<code>@mcp.tool()</code>装饰器就可以快速添加天气查询功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">from typing import Any</span><br><span class="line">from mcp.server.fastmcp import FastMCP</span><br><span class="line"></span><br><span class="line"># 初始化MCP服务器</span><br><span class="line">mcp = FastMCP(&quot;WeatherServer&quot;)</span><br><span class="line"></span><br><span class="line">@mcp.tool()</span><br><span class="line">async def get_weather(city: str):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    输入指定城市的名称，返回当前天气情况</span><br><span class="line">    :param city: 城市名称</span><br><span class="line">    :return: json格式的天气信息</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    url=&quot;https://api.seniverse.com/v3/weather/now.json&quot;</span><br><span class="line">    params=&#123;</span><br><span class="line">        &quot;key&quot;: &quot;注册的心知天气私钥&quot;,</span><br><span class="line">        &quot;location&quot;: city,</span><br><span class="line">        &quot;language&quot;: &quot;zh-Hans&quot;,</span><br><span class="line">        &quot;unit&quot;: &quot;c&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, params=params)</span><br><span class="line">    temperature = response.json()[&#x27;results&#x27;][0][&#x27;now&#x27;]</span><br><span class="line">    return json.dumps(temperature)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    mcp.run(transport=&quot;sse&quot;)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>不同之处</p>
</blockquote>
<p><code>MCP</code>和<code>Function Calling</code>两种技术目标都是为了增强大模型的外部交互能力，但<code>Function Calling</code>技术采用集中式架构，函数定义，模型调用和执行逻辑是<strong>紧密耦合</strong>在单一应用中的。设计简单直接，适合小型项目但难以拓展，一遇到复杂项目代码就变得很臃肿。</p>
<p><code>MCP</code>则采用了分布式架构，明确分离了MCP Server和MCP Client，很像现在开发项目的前后端分离模式，这种<strong>解耦</strong>使各组件能够独立演化，更适合企业级应用。</p>
<p>还有一点很重要，<code>Function Calling</code>通常采用同步执行模型，模型发起请求后等待响应，这在处理耗时操作时一定会导致性能瓶颈，比如当一个用户查询天气时，另一个用户只能排队等他查询完。<code>MCP</code>原生就支持异步操作，允许长时间任务在后台执行，通过回调机制通知用户结果。这种模式更符合真实业务场景，使AI能并行处理多个请求，显著提升吞吐量。</p>
<blockquote>
<p>client.py -&gt; 大模型客户端</p>
</blockquote>
<p>该客户端代码最大的优势就是完全没有绑定具体函数调用和大模型的逻辑，也就是说该客户端代码无论面对哪种类型的MCP Server都可以成功执行，这样在以后开发中可以保持MCP Client的代码不变，专注于快速开发或者访问不同的MCP Server，可以便捷的使用大模型开发不同能力的AI Agent智能体，这就是<code>MCP</code>相比于<code>Function Calling</code>最大的技术优势！分布式的架构，解耦的代码等于更便捷快速的开发速度。</p>
<p>许多桌面级大模型连接工具如cherry studio、vscode都集成了MCP，其实就是在现有工具的代码上加上一个<code>client.py</code>的调用模块，再自定义一下读取用户设置的MCP server，就能实现支持MCP的功能了，没有很复杂，MCP只是完整客户端支持的一个小模块，还有很多其他的模块，例如通过API连接远程LLM等。</p>
<p>对比<code>sse</code>客户端代码和<code>stdio</code>客户端代码的区别，可以发现只是在连接服务函数层面有变化（<code>connect_to_server (self, server_script_path)</code>或者 <code>connect_to_sse_server(self, server_url)</code>)。</p>
<ul>
<li><strong>LLM ↔ Host</strong>：OpenAI&#x2F;DeepSeek 的 SDK 自动把 <em>function&#x2F;tool calling</em> 的 JSON 打包&#x2F;解析。</li>
<li><strong>Host ↔ MCP Server</strong>：<code>mcp</code> 库里的 <code>ClientSession</code> 自动把调用转换成 <strong>JSON-RPC 2.0</strong> 请求并通过 stdio 传输；返回值也自动解析。</li>
</ul>
<p>所以看不到显式的 JSON 拼接是正常的， <strong>使用的代码只是在做“结构之间的映射”，不是在手写协议</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">import asyncio</span><br><span class="line">import json</span><br><span class="line">from typing import Optional</span><br><span class="line">from contextlib import AsyncExitStack</span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line">from mcp import ClientSession, StdioServerParameters</span><br><span class="line">from mcp.client.stdio import stdio_client</span><br><span class="line"></span><br><span class="line">class MCPClient:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;初始化MCP客户端&quot;&quot;&quot;</span><br><span class="line">        self.exit_stack = AsyncExitStack()</span><br><span class="line">        self.opanai_api_key = &quot;替换你的api_key&quot; # 调用模型的api_key</span><br><span class="line">        self.base_url = &quot;https://api.deepseek.com&quot; # 调用模型url, 这里以deepseek作演示</span><br><span class="line">        self.model = &quot;deepseek-chat&quot; # 调用deepseek-v3模型</span><br><span class="line">        self.client = OpenAI(api_key=self.opanai_api_key, base_url=self.base_url)</span><br><span class="line">        self.session: Optional[ClientSession] = None # Optional提醒用户该属性是可选的，可能为None</span><br><span class="line">        self.exit_stack = AsyncExitStack() # 用来存储和清楚对话中上下文的，提高异步资源利用率</span><br><span class="line"></span><br><span class="line">    async def connect_to_server(self, server_script_path):	# stdio的server，因为是本地文件，需要传入本地代码的路径</span><br><span class="line">        &quot;&quot;&quot;连接到MCP服务器并列出MCP服务器的可用工具函数&quot;&quot;&quot;</span><br><span class="line">        server_params = StdioServerParameters(</span><br><span class="line">        command=&quot;python&quot;,</span><br><span class="line">            args=[server_script_path],</span><br><span class="line">            env=None</span><br><span class="line">        ) # 设置启动服务器的参数, 这里是要用python执行server.py文件</span><br><span class="line"></span><br><span class="line">        # 启动MCP服务器并建立通信</span><br><span class="line">        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))</span><br><span class="line">        self.stdio, self.write = stdio_transport</span><br><span class="line">        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))</span><br><span class="line"></span><br><span class="line">        await self.session.initialize() # 与服务器建立stdio连接</span><br><span class="line"></span><br><span class="line">        # 列出MCP服务器上的工具</span><br><span class="line">        response = await self.session.list_tools()</span><br><span class="line">        tools = response.tools</span><br><span class="line">        print(&quot;\n已连接到服务器，支持以下工具:&quot;, [tool.name for tool in tools])#打印服务端可用的工具</span><br><span class="line">  </span><br><span class="line">    async def process_query(self, query:str)-&gt;str:</span><br><span class="line">        &quot;&quot;&quot;使用大模型处理查询并调用MCP Server可用的MCP工具&quot;&quot;&quot;</span><br><span class="line">        messages = [&#123;&quot;role&quot;:&quot;user&quot;, &quot;content&quot;:query&#125;]</span><br><span class="line">        response = await self.session.list_tools()</span><br><span class="line"></span><br><span class="line">        available_tools = [&#123;</span><br><span class="line">            &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">            &quot;function&quot;: &#123;</span><br><span class="line">                &quot;name&quot;: tool.name,</span><br><span class="line">                &quot;description&quot;: tool.description,</span><br><span class="line">                &quot;input_schema&quot;: tool.inputSchema</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; for tool in response.tools]</span><br><span class="line"></span><br><span class="line">        response = self.client.chat.completions.create(</span><br><span class="line">            model=self.model,</span><br><span class="line">            messages=messages,</span><br><span class="line">            tools=available_tools</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 处理返回内容</span><br><span class="line">        content = response.choices[0]</span><br><span class="line">        if content.finish_reason == &quot;tool_calls&quot;:</span><br><span class="line">            # 返回结果是使用工具的建议，就解析并调用工具</span><br><span class="line">            tool_call = content.message.tool_calls[0]</span><br><span class="line">            tool_name = tool_call.function.name</span><br><span class="line">            tool_args = json.loads(tool_call.function.arguments)</span><br><span class="line">            # 执行工具</span><br><span class="line">            result = await self.session.call_tool(tool_name, tool_args)</span><br><span class="line">            print(f&quot;\n\n[Calling tool &#123;tool_name&#125; with args &#123;tool_args&#125;]\n\n&quot;)</span><br><span class="line">            # 将模型返回的调用工具的对话记录保存在messages中</span><br><span class="line">            messages.append(content.message.model_dump())</span><br><span class="line">            messages.append(&#123;</span><br><span class="line">                &quot;role&quot;: &quot;tool&quot;,</span><br><span class="line">                &quot;content&quot;: result.content[0].text,</span><br><span class="line">                &quot;tool_call_id&quot;: tool_call.id,</span><br><span class="line">            &#125;)</span><br><span class="line">            # 将上面的结果返回给大模型用于生产最终结果</span><br><span class="line">            response = self.client.chat.completions.create(</span><br><span class="line">                model=self.model,</span><br><span class="line">                messages=messages</span><br><span class="line">            )</span><br><span class="line">            return response.choices[0].message.content</span><br><span class="line">        return content.message.content</span><br><span class="line">  </span><br><span class="line">    async def chat_loop(self):</span><br><span class="line">        &quot;&quot;&quot;运行交互式聊天&quot;&quot;&quot;</span><br><span class="line">        print(&quot;\n MCP客户端已启动！输入quit退出&quot;)</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            try:</span><br><span class="line">                query = input(&quot;\n用户:&quot;).strip()</span><br><span class="line">                if query.lower() == &#x27;quit&#x27;:</span><br><span class="line">                    break</span><br><span class="line">                response = await self.process_query(query)</span><br><span class="line">                print(f&quot;\nDeepSeek-V3-0324: &#123;response&#125;&quot;)</span><br><span class="line">            except Exception as e:</span><br><span class="line">                print(f&quot;发生错误: &#123;str(e)&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    async def clean(self):</span><br><span class="line">        &quot;&quot;&quot;清理资源&quot;&quot;&quot;</span><br><span class="line">        await self.exit_stack.aclose()</span><br><span class="line"></span><br><span class="line">async def main():</span><br><span class="line">    if len(sys.argv) &lt; 2:</span><br><span class="line">        print(&quot;使用方法是： python client.py server.py&quot;)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    client = MCPClient()</span><br><span class="line">    try:</span><br><span class="line">        await client.connect_to_server(sys.argv[1])</span><br><span class="line">        await client.chat_loop()</span><br><span class="line">    finally:</span><br><span class="line">        await client.clean()</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    import sys</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure>

<blockquote>
<p>server.py</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from mcp.server.fastmcp import FastMCP</span><br><span class="line"></span><br><span class="line"># 创建MCP 服务</span><br><span class="line">mcp = FastMCP(&#x27;Demo&#x27;)</span><br><span class="line"></span><br><span class="line">@mcp.tool()</span><br><span class="line">def add(a:int, b:int) -&gt;int:</span><br><span class="line">  &quot;&quot;&quot;&quot;</span><br><span class="line">  计算两个整数的和并返回</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  return a+b</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">  # 以标准 I/O 方式运行 MCP 服务器</span><br><span class="line">  mcp.run(transport=&#x27;stdio&#x27;)</span><br></pre></td></tr></table></figure>

<h2 id="MCP-Host与LLM之间的通信"><a href="#MCP-Host与LLM之间的通信" class="headerlink" title="MCP Host与LLM之间的通信"></a>MCP Host与LLM之间的通信</h2><blockquote>
<p>LLM模型永远只能接受一段文本,是的,只有一段,无法并行读取,也无法分开,不管任何技术都是合并到一堆字符串里去,能不能完美理解出来全看LLM的智能程度</p>
</blockquote>
<p>大模型（如 GPT-4&#x2F;5）在返回推理结果时，只会输出<strong>结构化的 JSON</strong>，这就是 <strong>function calling &#x2F; tool calling</strong> 的产物。本身不会直接发 HTTP 或 RPC 请求去调用 MCP Server，它只是根据上下文生成或提示词生成“工具调用”的 JSON 结构化输出。</p>
<ul>
<li>在 OpenAI API 里，就是 <strong>function_call &#x2F; tool_call</strong> 的 JSON。</li>
<li>在 Anthropic、Claude、Gemini 里也有类似机制。</li>
</ul>
<p><strong>MCP Host 的作用相当于一个翻译器</strong>。LLM 把 JSON 格式化结果交给 Host，Host 内部的 <strong>MCP Client</strong> 模块会把它转换成 <strong>MCP 规定的 JSON-RPC 2.0 请求</strong>，这才是 MCP Server 能识别的标准调用。可以这么理解：</p>
<ul>
<li><strong>function calling JSON</strong> 是 LLM 与 MCP Host 的接口格式；</li>
<li><strong>JSON-RPC</strong> 则是 MCP Client 与 MCP Server 的接口格式。</li>
</ul>
<p><code>JSON-RPC 2.0</code> 请求会通过一个 <strong>传输层（Transport）</strong> 发给 MCP Server，传输层可以是：</p>
<ul>
<li><strong>stdio</strong>（进程间标准输入输出，最常见，延迟低）</li>
<li><strong>SSE</strong>（跨网络时）</li>
<li><strong>Streamable HTTP</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">``` LLM输出</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tool_call&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;query_database&quot;,</span><br><span class="line">    &quot;arguments&quot;: &#123;</span><br><span class="line">      &quot;sql&quot;: &quot;SELECT * FROM users LIMIT 10&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">JSON-RPC 2.0 格式</span><br><span class="line">&#123;</span><br><span class="line">  &quot;jsonrpc&quot;: &quot;2.0&quot;,</span><br><span class="line">  &quot;id&quot;: 42,</span><br><span class="line">  &quot;method&quot;: &quot;tools/call&quot;,</span><br><span class="line">  &quot;params&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;query_database&quot;,</span><br><span class="line">    &quot;arguments&quot;: &#123;</span><br><span class="line">      &quot;sql&quot;: &quot;SELECT * FROM users LIMIT 10&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="MCP-Client与MCP-Server的三种通信机制对比"><a href="#MCP-Client与MCP-Server的三种通信机制对比" class="headerlink" title="MCP Client与MCP Server的三种通信机制对比"></a>MCP Client与MCP Server的三种通信机制对比</h1><h2 id="STDIO"><a href="#STDIO" class="headerlink" title="STDIO"></a>STDIO</h2><p><strong><code>STDIO</code>（标准输入输出）是一种用于本地通信的传输方式</strong>。其核心思想是利用操作系统提供的管道机制实现父子进程间的数据交换，客户端通过标准输入发送请求，服务器通过标准输出返回响应。<strong>在这种模式下，客户端(MCP-Client)通过启动MCP Server可执行文件作为子进程</strong>， 双方通过约定的标准输入和标准输出进行数据交换。</p>
<p><strong>这种方式适用于MCP-Client和MCP-Server都在同一台计算机上的场景</strong>，例如本地自行编写服务端代码或将别人编写的服务端代码pull到本地执行。设计极其简单直接，完全依赖操作系统内核提供的进程管理能力。无需配置网络环境，数据仅在本地流转，避免远程泄露风险，直接通过内存或管道传输，确保了高效、低延迟的通信性能；但无法同时处理多客户端请求，扩展性差，无法支持分布式或远程资源访问。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250727163108081.png" alt="STDIO传输方式"></p>
<h2 id="SSE-已停止，官网更新为Streamable-HTTP"><a href="#SSE-已停止，官网更新为Streamable-HTTP" class="headerlink" title="SSE(已停止，官网更新为Streamable HTTP)"></a>SSE(已停止，官网更新为Streamable HTTP)</h2><p><strong>SSE（Server-Sent Events，服务器发送事件） 是一种基于 HTTP 协议的单向通信机制，由服务器主动向浏览器单向推送数据;</strong> SSE是解决“HTTP请求只能由客户端主动向服务器发消息，不能由服务器主动向客户端发消息”问题的一种方案；就像一条单向的传送带，只能由服务器向客户端发送数据，客户端无法通过这条传送带将物品送回服务器，适合需要服务器持续向客户端更新的场景。</p>
<p>要注意SSE协议和WebSocket协议的区别，<strong>SSE协议是单向的，客户端和服务端建立连接后，只能由服务端向客户端进行消息推送。</strong>而WebSocket协议是全双工的，客户端和服务端建立连接后，可以双向通信。</p>
<p>从架构层面看，<code>SSE</code>构建在标准的<code>HTTP</code>协议之上，利用其长连接特性实现服务器到客户端的单向实时数据流，这种设计使其天然适合分布式系统和网络化部署场景， 适用于客户端和服务端位于不同物理位置的场景，尤其是对于分布式或远程部署的场景。<code>SSE</code> 对网络波动的适应性不强，一旦网络不太稳定，数据传输就容易卡顿甚至中断。</p>
<p>MCP Client和MCP Server 在HTTP SSE方式下通过两个主要通道进行通信：</p>
<ul>
<li>HTTP请求&#x2F;响应：客户端通过标准HTTP请求发送消息到服务端（一般是POST，由client先发送一个post请求到服务器）</li>
<li>服务器推送事件（SSE）：通过专门的<code>/sse</code>端点向客户端推送消息</li>
</ul>
<blockquote>
<p>SSE的不足</p>
</blockquote>
<p>虽然SSE这种设计方式简单直观，但它存在如下关键问题：</p>
<ol>
<li>不支持断线重连&#x2F;恢复：SSE连接断开所有会话状态丢失，客户端必须重新建立连接并初始化整个会话。</li>
<li>服务器需维护长连接：服务器必须为每个客户端维护一个长时间的SSE连接，大量并发用户会导致资源消耗剧增，当服务器重启或扩容时所有连接会中断影响用户体验和系统可靠性。</li>
<li>服务器消息只能通过SSE传递：即使是简单的请求-响应交互、服务器也必须通过SSE通道返回信息，这就需要服务器一直保持SSE连接，造成不必要的复杂性和开销。</li>
<li>基础设施兼容性限制：目前很多Web基础设施如CDN、负载均衡器、API网关等对长时间SSE连接支持性不够，企业防火墙有可能强制关闭超时SSE连接，造成连接不可用。</li>
</ol>
<h2 id="Streamable-HTTP"><a href="#Streamable-HTTP" class="headerlink" title="Streamable HTTP"></a>Streamable HTTP</h2><p>在2025年3月26日，MCP官方<code>Github仓库</code>出现了采用“可流式传输的 HTTP”来替代现有的 HTTP+SSE 方案的<code>Streamable HTTP</code>提议（issue）。该提议详细说明了<strong>Streamable HTTP MCP</strong> 服务器与客户端之间的通信流程，以及外部工具调用信息同步格式与流程；</p>
<p>2025年5月9日，MCP 迎来重磅升级——<strong>Streamable HTTP</strong>正式发布，取代了<strong>HTTP SSE</strong>, 成为AI模型通信的新标准！</p>
<p>在<strong>Streamable HTTP</strong>传输中，服务器作为独立进程运行，可以处理多个客户端连接。传输使用 HTTP POST 和 GET 请求。服务器可以选择使用服务器发送事件(SSE) 来传输多条服务器消息。这不仅允许基本的 MCP 服务器，还允许功能更丰富的服务器支持流式传输以及服务器到客户端的通知和请求。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250606015213240.png" alt="流程图"></p>
<blockquote>
<p>什么是Streamable HTTP？</p>
</blockquote>
<p>“<strong>Streamable HTTP</strong>” 是一种基于标准 HTTP 协议的 <strong>流式传输机制</strong>，允许服务器在保持连接不断开的前提下，<strong>持续&#x2F;分批地发送数据</strong>给客户端。这种模式适用于需要<strong>实时数据推送或大数据逐步传输</strong>的场景。</p>
<p><code>Streamable HTTP</code> 是为了解决传统 SSE（Server-Sent Events）传输方式的局限性而提出的。<code>SSE</code> 需要长连接，且仅支持单向通信（服务器到客户端），导致其在某些场景下不够灵活。<code>Streamable HTTP</code> 则通过改进传输机制，支持更灵活的双向通信和无状态服务器运行。</p>
<blockquote>
<p><code>Streamable HTTP</code> 的核心特性</p>
</blockquote>
<ul>
<li>支持流式传输：<code>Streamable HTTP</code> 允许数据以流的形式分块传输，而不是一次性传输完整数据。这种方式可以减少客户端和服务器的内存占用，并提高实时性。</li>
<li>无状态服务器：服务器可以选择完全无状态运行，不再需要维持长期连接。这使得服务器资源利用率更高，更适合高并发场景。</li>
<li>兼容性与易用性：<code>Streamable HTTP</code> 基于标准的 <code>HTTP</code> 协议，兼容现有的 HTTP 基础设施，包括 <code>CDN</code>、<code>API</code> 网关和负载均衡等。</li>
</ul>
<blockquote>
<p>解决了SSE哪些问题？</p>
</blockquote>
<ol>
<li><strong>Streamable HTTP</strong>如何解决<strong>SSE</strong>不支持断线重连的问题？</li>
</ol>
<p>​	答：<strong>Streamable HTTP</strong>在每次通信时会记录id编号对应请求与响应，将这里请求与响应存储可断线重连进行恢复。</p>
<ol start="2">
<li><strong>Streamable HTTP</strong>如何解决<strong>SSE</strong>服务器需要维持长连接的问题？</li>
</ol>
<p>​	答：在需要发送响应过程中会保持连接，但一旦流式响应结束，服务器随后便会关闭流。</p>
<ol start="3">
<li><strong>Streamable HTTP</strong>如何解决<strong>SSE</strong>服务器消息只能通过SSE传输的问题？</li>
</ol>
<p>​	<strong>Streamable HTTP</strong>服务器可灵活选择是返回普通HTTP响应还是升级为SSE流，对于简单请求直接使用普通HTTP响应，对于内容复杂等需要流式传输的请求场景自动升级为SSE。</p>
<ol start="4">
<li><strong>Streamable HTTP</strong>如何解决<strong>SSE</strong>服务器基础设施兼容性限制？</li>
</ol>
<p>​	答：<strong>Streamable HTTP</strong>各基础设施的兼容性很完备。</p>
<h2 id="三种方式对比"><a href="#三种方式对比" class="headerlink" title="三种方式对比"></a>三种方式对比</h2><table>
<thead>
<tr>
<th>特性</th>
<th>Stdio</th>
<th>SSE</th>
<th>Streamable HTTP</th>
</tr>
</thead>
<tbody><tr>
<td>通信方式</td>
<td>本地进程管道</td>
<td>HTTP 长连接 + SSE</td>
<td>标准 HTTP + 动态流式升级</td>
</tr>
<tr>
<td>适用场景</td>
<td>本地隐私数据处理</td>
<td>实时远程通知</td>
<td>云原生、分布式系统</td>
</tr>
<tr>
<td>网络依赖</td>
<td>无</td>
<td>必需</td>
<td>必需</td>
</tr>
<tr>
<td>多客户端支持</td>
<td>否</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>协议演进</td>
<td>长期支持</td>
<td>已废弃</td>
<td>官方推荐替代方案</td>
</tr>
</tbody></table>
<h1 id="当前环境下MCP的不足之处"><a href="#当前环境下MCP的不足之处" class="headerlink" title="当前环境下MCP的不足之处"></a>当前环境下MCP的不足之处</h1><h2 id="安全问题"><a href="#安全问题" class="headerlink" title="安全问题"></a>安全问题</h2><ul>
<li><strong>认证机制混乱</strong>：缺乏统一认证标准：部分服务器采用OAuth 2.0 + MFA，部分甚至无API密钥保护。权限过大可能会损害数据安全，如误删本地文件等。</li>
<li><strong>本地执行风险</strong>：通过 <code>stdio</code> 模式安装的第三方工具可能携带恶意代码（如伪装成PDF解析工具上传用户 <code>.bash_history</code> 文件）。</li>
</ul>
<h2 id="执行延迟问题"><a href="#执行延迟问题" class="headerlink" title="执行延迟问题"></a>执行延迟问题</h2><ul>
<li><p><strong>协议开销导致高延迟</strong>：MCP在处理高并发请求时，协议本身的复杂性（如多层通信架构、数据序列化）会引入显著延迟。在高负载场景下，其整体吞吐量甚至可能低于传统函数调用。尤其在边缘计算或分布式系统中，网络通信延迟会进一步放大这一问题。</p>
</li>
<li><p><strong>上下文管理效率低下</strong>：<strong>工具描述、返回的JSON数据等占用大量上下文窗口，挤压模型推理空间。</strong>实验表明，当工具数量从5个增至20个时，模型指令遵循准确率从78%暴跌至34%，形成“上下文污染”的恶性循环。</p>
</li>
</ul>
<h2 id="大模型输出错误缺陷"><a href="#大模型输出错误缺陷" class="headerlink" title="大模型输出错误缺陷"></a>大模型输出错误缺陷</h2><ul>
<li><strong>错误传播与“幻觉”放大</strong>：当工具返回部分错误数据时，大模型的“脑补”能力会合理化错误，导致更危险的输出。例如：医疗AI将仪器“-1”错误码解读为“检测值偏低”，生成错误诊断建议等。</li>
<li><strong>工具选择与参数解析错误</strong>：模型常混淆功能相似的工具（如混淆 <code>search_flights</code> 与 <code>query_timetable</code>）。</li>
<li><strong>复杂逻辑处理失败</strong>：（如时区转换错误），在结构化任务（如航班预订）中成功率不足20%。</li>
</ul>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>MCP协议并没有规定MCP Host如何与 LLM 之间进行通信，只是规定了MCP Client与MCP Server之间的通信方式，至于和AI大模型之间的通信方式，常见的有两种方式：</p>
<ul>
<li>可以直接用最原始的方式将工具说明书直接写到提示词里（简单粗暴），这种方法的好处是理论上来说任何指令遵循能力好大模型都可以使用MCP技术，但是可能会导致token爆炸，尤其是工具库包含很多工具时，可能会一次性传好几万token给大模型，如果LLM的API是按token计费方式则成本高昂；</li>
<li>也可以用<code>Function Calling</code>的方式传递将工具列表传递给AI大模型，但是对大模型有要求，必须支持Function Calling技术的大模型才行，本质上应该是模型底层实现了一种基于<code>Function Calling</code>的提示词技术，但是使用<code>Function Calling</code>是不计算在用户输入的token中的，应该还是离不开提示词技术。</li>
</ul>
<p>成熟一点的MCP与LLM连接方案本质上还是通过调用模型的<code>Function Calling</code>功能实现的，只是将MCP Server代码端的注释、参数、返回值等<code>json_schema</code>通过MCP 的SDK进行转换，再填充到<code>Function Calling</code>的待补充参数之中，可从下面的示例代码中略窥一二。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">async def process_query(self, query:str)-&gt;str:</span><br><span class="line">        &quot;&quot;&quot;使用大模型处理查询并调用MCP Server可用的MCP工具&quot;&quot;&quot;</span><br><span class="line">        messages = [&#123;&quot;role&quot;:&quot;user&quot;, &quot;content&quot;:query&#125;]</span><br><span class="line">        response = await self.session.list_tools()</span><br><span class="line"></span><br><span class="line">        available_tools = [&#123;</span><br><span class="line">            &quot;type&quot;: &quot;function&quot;,	# 还是离不开Function Calling</span><br><span class="line">            &quot;function&quot;: &#123;</span><br><span class="line">                &quot;name&quot;: tool.name,</span><br><span class="line">                &quot;description&quot;: tool.description,</span><br><span class="line">                &quot;input_schema&quot;: tool.inputSchema</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; for tool in response.tools]</span><br><span class="line"></span><br><span class="line">        response = self.client.chat.completions.create(	</span><br><span class="line">            model=self.model,</span><br><span class="line">            messages=messages,</span><br><span class="line">            tools=available_tools	# 把MCP对齐后的Function Calling传到工具列表中，还是用到Function Calling</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>MCP相关知识以及三种通信机制对比</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.liuhengfeng.xyz/posts/d33d6318.html">https://www.liuhengfeng.xyz/posts/d33d6318.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>LHF</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2025-06-06</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2025-07-27</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a></div><div class="post_share"><div class="social-share" data-image="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-06-06_00-40-46.jpg" data-sites="twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/e392761a.html" title="提示词工程/调优"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-06-07_15-11-45.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">提示词工程/调优</div></div></a></div><div class="next-post pull-right"><a href="/posts/9a97f904.html" title="2025-06"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/lock.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2025-06</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/796b2fbf.html" title="2025的Agent共识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-07-11_22-03-45.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-11</div><div class="title">2025的Agent共识</div></div></a></div><div><a href="/posts/2ca90abc.html" title="AI基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/bc18e18a-2e2d-4378-9c15-f1c678fc5ed1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-19</div><div class="title">AI基础知识</div></div></a></div><div><a href="/posts/1a411b1e.html" title="AutoDL服务器常用命令记录"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-18_14-41-41.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-18</div><div class="title">AutoDL服务器常用命令记录</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MCP%E5%85%88%E5%AF%BC%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">MCP先导知识</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFMCP%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">什么是MCP？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MCP%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">2.1.</span> <span class="toc-text">MCP架构与工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MCP%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.2.</span> <span class="toc-text">MCP应用场景</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#JSON-RPC-2-0-%E5%8D%8F%E8%AE%AE"><span class="toc-number">3.</span> <span class="toc-text">JSON-RPC 2.0 协议</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MCP-%E4%B8%8E-Function-Calling-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">4.</span> <span class="toc-text">MCP 与 Function Calling 的关系</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MCP-Host%E4%B8%8ELLM%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1"><span class="toc-number">4.1.</span> <span class="toc-text">MCP Host与LLM之间的通信</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MCP-Client%E4%B8%8EMCP-Server%E7%9A%84%E4%B8%89%E7%A7%8D%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6%E5%AF%B9%E6%AF%94"><span class="toc-number">5.</span> <span class="toc-text">MCP Client与MCP Server的三种通信机制对比</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#STDIO"><span class="toc-number">5.1.</span> <span class="toc-text">STDIO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSE-%E5%B7%B2%E5%81%9C%E6%AD%A2%EF%BC%8C%E5%AE%98%E7%BD%91%E6%9B%B4%E6%96%B0%E4%B8%BAStreamable-HTTP"><span class="toc-number">5.2.</span> <span class="toc-text">SSE(已停止，官网更新为Streamable HTTP)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Streamable-HTTP"><span class="toc-number">5.3.</span> <span class="toc-text">Streamable HTTP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="toc-number">5.4.</span> <span class="toc-text">三种方式对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%8E%AF%E5%A2%83%E4%B8%8BMCP%E7%9A%84%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84"><span class="toc-number">6.</span> <span class="toc-text">当前环境下MCP的不足之处</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98"><span class="toc-number">6.1.</span> <span class="toc-text">安全问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98"><span class="toc-number">6.2.</span> <span class="toc-text">执行延迟问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E9%94%99%E8%AF%AF%E7%BC%BA%E9%99%B7"><span class="toc-number">6.3.</span> <span class="toc-text">大模型输出错误缺陷</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">7.</span> <span class="toc-text">补充</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-wrap" id="footer-wrap"> <div class="copyright">&copy;2024 - 2025 By LHF</div><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script src="https://statics.liuhengfeng.xyz/js/lunar.min.js"></script><script src="https://statics.liuhengfeng.xyz/js/calendar.min.js"></script><script src="https://statics.liuhengfeng.xyz/js/calendar.boot.js"></script><script src="https://statics.liuhengfeng.xyz/js/jquery-3.6.1.min.js"></script><script async src="https://unpkg.com/vue@2.6.14/dist/vue.min.js"></script><script async src="https://unpkg.com/element-ui@2.15.6/lib/index.js"></script><script async src="https://statics.liuhengfeng.xyz/js/common.min.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false#  open shake (抖動特效);
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('card-widget card-announcement')[2];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = 'f89c5a6b949a4345a0d22f0ad5b040ea';
  var gaud_map_key = 'a36227a8c2b6dcd6b90a98920a06ac89';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '108.230574,22.793414';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://devapi.qweather.com/v7/weather/now?key=f89c5a6b949a4345a0d22f0ad5b040ea"></script><script data-pjax src="https://liu-hexo.oss-cn-guangzhou.aliyuncs.com/js/electric_clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="博客主题为Butterfly" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-d021d6?style=flat&amp;logo=buefy&amp;color=purple" alt=""/></a><a class="github-badge" target="_blank" href="https://pages.github.com/" style="margin-inline:5px" data-title="本站项目托管于Github Pages" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github%20Pages-d021d6?style=flat&amp;logo=githubpages&amp;color=brown" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="GitHub万岁" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Thanks-Github-d021d6?style=flat&amp;logo=GitHub&amp;color=indigo" alt=""/></a><a class="github-badge" target="_blank" href="https://www.alibabacloud.com/zh/product/content-delivery-network?_p_lc=1" style="margin-inline:5px" data-title="网站资源使用阿里云CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-%E9%98%BF%E9%87%8C%E4%BA%91CDN-d021d6?style=flat&amp;logo=speedtest&amp;color=blue" alt=""/></a><a class="github-badge" target="_blank" href="https://www.aliyun.com/benefit" style="margin-inline:5px" data-title="本站图床部署于阿里云OSS" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/OSS-%E9%98%BF%E9%87%8C%E4%BA%91OSS-d021d6?style=flat&amp;logo=alibabacloud&amp;color=orange" alt=""/></a><a class="github-badge" target="_blank" href="https://beian.miit.gov.cn" style="margin-inline:5px" data-title="网站备案号" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/ICP-%E6%A1%82ICP%E5%A4%872023012461%E5%8F%B7-d021d6?style=flat&amp;logo=brandfolder&amp;color=yellow" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=creativecommons" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInRight');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('footer-wrap');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__backInUp');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('show');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInDown');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('is-center');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-data');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-social-icons');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-btn');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>
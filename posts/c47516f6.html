<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Note_of_Learning(CS&amp;AI) | Liu's  Blog</title><meta name="author" content="LHF"><meta name="copyright" content="LHF"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一些笔记杂记，主要是AI方面的">
<meta property="og:type" content="article">
<meta property="og:title" content="Note_of_Learning(CS&amp;AI)">
<meta property="og:url" content="https://liuhengfeng.xyz/posts/c47516f6.html">
<meta property="og:site_name" content="Liu&#39;s  Blog">
<meta property="og:description" content="一些笔记杂记，主要是AI方面的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-10_16-40-48.webp">
<meta property="article:published_time" content="2025-05-10T08:45:17.000Z">
<meta property="article:modified_time" content="2025-05-10T08:45:17.000Z">
<meta property="article:author" content="LHF">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-10_16-40-48.webp"><link rel="shortcut icon" href="https://statics.liuhengfeng.xyz/Hexo/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240907191402.jpg"><link rel="canonical" href="https://liuhengfeng.xyz/posts/c47516f6.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":290},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Note_of_Learning(CS&AI)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-10 16:45:17'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/custom.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/universe.css"><span id="fps"></span><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/hexo_electric_clock.css"><link rel="stylesheet" href="/css/bottom_runtime.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/lib/theme-chalk/index.css"><link rel="stylesheet" href="/css/calendar.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/css/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://statics.liuhengfeng.xyz/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/u%3D1968668429%2C2104382916%26fm%3D253%26fmt%3Dauto%26app%3D138%26f%3DJPEG.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-10_16-40-48.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="Liu's  Blog"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/2876f803-ae9a-49ac-bcd9-9d623c5beb03.jpg"/><span class="site-name">Liu's  Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Note_of_Learning(CS&amp;AI)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-10T08:45:17.000Z" title="发表于 2025-05-10 16:45:17">2025-05-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-10T08:45:17.000Z" title="更新于 2025-05-10 16:45:17">2025-05-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Note_of_Learning(CS&amp;AI)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LangChain-v0-3-x与python版本问题"><a href="#LangChain-v0-3-x与python版本问题" class="headerlink" title="LangChain v0.3.x与python版本问题"></a>LangChain v0.3.x与python版本问题</h1><blockquote>
<p>Python 3.8无法使用最新的LangChain v0.3.x，为了获得最佳兼容性，避免潜在问题，推荐使用3.9及其以上的Python版本</p>
</blockquote>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在conda中安装langchain及其相关组件时，发现无法安装最新版的<code>LangChain v0.3.x</code>，而是相对过时的<code>LangChain v0.2.x</code>，就算使用下面的命令强制安装最新版得到的也只是0.2.x的版本；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade langchain langchain-community langchain-core langchain-openai langchain-text-splitters langsmith</span><br></pre></td></tr></table></figure>

<p>当指定版本时也无法找到对应版本的安装包，<code>ERROR: No matching distribution found for langchain==0.3.25</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade --force-reinstall langchain==0.3.25</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250510170455769.png" alt="无法找到对应版本包"></p>
<p>查阅<code>LangChain v0.3</code>的官网，发现<code>LangChain v0.3</code>对python版本是有要求的，在2024 年 10 月之前可能是支持python3.8的，但是现在是2025-05-10，python3.8已经不受官方支持了，要求python≥3.9，所以如果要使用最新版的LangChain v0.3，要求python版本≥3.9，我现在的虚拟环境是python3.8，不符合要求所以无法安装最新版的模块包</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250510170724761.png" alt="官网对python版本要求"></p>
<blockquote>
<p>不同python版本对应的的LangChain </p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250510165845444.png" alt="python3.8版本的langchain"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250510165746359.png" alt="python3.9的langchain"></p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol>
<li>在anaconda中升级python版本，将原来的python3.8升级到python3.9版本</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.10</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>升级langchain及其相关组件的版本；使用下列命令会尝试升级所列出的这几个langchain包到当前环境和可用软件源下能找到的最新兼容版本，为了满足这些核心包新版本的依赖要求，它也会升级其依赖的包，只处理指定的以及为了满足指定包的依赖而必须升级的包，不会影响没有依赖的包，其他项目指定版本的包也还能正常使用</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade langchain langchain-community langchain-core langchain-openai langchain-text-splitters langsmith</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/image-20250510171826153.png" alt="langchain升级完成"></p>
<h1 id="Python语法"><a href="#Python语法" class="headerlink" title="Python语法"></a>Python语法</h1><h2 id="三元表达式"><a href="#三元表达式" class="headerlink" title="三元表达式"></a>三元表达式</h2><p>三元表达式提供了一种简洁的方式来根据条件在两个值之间进行选择。它不是一个运算符，而是一种条件表达式（Conditional Expression）。之所以叫“三元”表达式，是因为它涉及到三个操作数：<strong>一个条件、条件为真时的结果、条件为假时的结果</strong>。主要用于在一个简单的 <code>if-else</code> 逻辑中，根据条件给一个变量赋值，或者在一个表达式中直接根据条件得到一个值。相比传统的 <code>if-else</code> 语句，它更紧凑，适合简单的场景。</p>
<p>语法格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value_if_true if condition else value_if_false</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>value_if_true</code>: 如果 <code>condition</code> 为 <code>True</code>，整个表达式的值就是这个。</p>
</li>
<li><p><code>if</code>: 关键字。</p>
</li>
<li><p><code>condition</code>: 一个布尔表达式，它的结果是 <code>True</code> 或 <code>False</code>。</p>
</li>
<li><p><code>else</code>: 关键字。</p>
</li>
<li><p><code>value_if_false</code>: 如果 <code>condition</code> 为 <code>False</code>，<strong>整个表达式的值就是这个。</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 示例1：找出两个数中的最大值</span><br><span class="line"># 传统if-else结构</span><br><span class="line">a = 15</span><br><span class="line">b = 8</span><br><span class="line">if a &gt; b:</span><br><span class="line">    maximum = a</span><br><span class="line">else:</span><br><span class="line">    maximum = b</span><br><span class="line">print(maximum) # 输出: 15</span><br><span class="line"></span><br><span class="line"># 三元表达式结构</span><br><span class="line">a = 15</span><br><span class="line">b = 8</span><br><span class="line">maximum = a if a &gt; b else b</span><br><span class="line">print(maximum) # 输出: 15</span><br><span class="line"></span><br><span class="line"># 示例二：api_key赋值，这里用括号()是为了更美观，就像是复杂条件用括号分开以便更好地阅读</span><br><span class="line">   api_key = (</span><br><span class="line">        st.session_state.api_key</span><br><span class="line">        if &quot;api_key&quot; in st.session_state and st.session_state.api_key != &quot;&quot;</span><br><span class="line">        else None</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line"># 例子三：在函数返回值中使用</span><br><span class="line">def get_status(score):</span><br><span class="line">    # 如果分数大于等于60，返回&quot;及格&quot;，否则返回&quot;不及格&quot;</span><br><span class="line">    return &quot;及格&quot; if score &gt;= 60 else &quot;不及格&quot;</span><br><span class="line"></span><br><span class="line">print(get_status(75)) # 输出: 及格</span><br><span class="line">print(get_status(50)) # 输出: 不及格</span><br></pre></td></tr></table></figure>

<h2 id="海象运算符（Walrus-Operator）"><a href="#海象运算符（Walrus-Operator）" class="headerlink" title="海象运算符（Walrus Operator）"></a>海象运算符（Walrus Operator）</h2><p>海象运算符是 Python 3.8 引入的一个新特性，正式名称是赋值表达式（Assignment Expression），符号是 <code>:=</code>（因为 <code>:=</code> 的形状有点像海象的眼睛和獠牙），作用是可以在表达式内部对变量赋值，同时返回该值</p>
<p>语法格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">variable := expression</span><br></pre></td></tr></table></figure>

<ul>
<li><code>variable</code>：要赋值的变量</li>
<li><code>expression</code>：计算结果将被赋值给变量的表达式</li>
<li>整个 :&#x3D; 表达式的值是 expression 的结果</li>
</ul>
<p>在没有海象运算符之前，如果想在表达式中使用一个值并同时保存它，通常需要两步操作。海象运算符允许一步完成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 示例一：传统写法，计算一个值并使用它</span><br><span class="line">result = len(&quot;hello&quot;)</span><br><span class="line">if result &gt; 3:</span><br><span class="line">    print(result)</span><br><span class="line">    </span><br><span class="line"># 使用海象运算符，直接在表达式中赋值和使用</span><br><span class="line">if (result := len(&quot;hello&quot;)) &gt; 3:</span><br><span class="line">    print(result)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 示例二：传统写法，处理文件或列表时</span><br><span class="line">with open(&quot;file.txt&quot;) as f:</span><br><span class="line">    content = f.read()</span><br><span class="line">    if len(content) &gt; 0:</span><br><span class="line">        print(content)</span><br><span class="line"></span><br><span class="line"># 使用海象运算符</span><br><span class="line">with open(&quot;file.txt&quot;) as f:</span><br><span class="line">    if (content := f.read()) and len(content) &gt; 0:</span><br><span class="line">        print(content)</span><br></pre></td></tr></table></figure>

<h2 id="多行字符串-的作用"><a href="#多行字符串-的作用" class="headerlink" title="多行字符串&#39;&#39;&#39; 的作用"></a>多行字符串<code>&#39;&#39;&#39;</code> 的作用</h2><p>在 Python 中，<code>&#39;&#39;&#39;</code>（或 <code>&quot;&quot;&quot;</code>）可以用来表示多行字符串，但它们是否被视为注释取决于上下文。LangChain 中经常使用多行字符串作为提示词（Prompt）内容，而不是单行字符串（如 “”）</p>
<p>Python 中 <code>&#39;&#39;&#39;</code> 的作用多行字符串，不是直接的注释；<code>&#39;&#39;&#39;</code>（三单引号）或 <code>&quot;&quot;&quot;</code>（三双引号）用于定义<strong>多行字符串</strong>，而不是专门的注释符号（# 才是专门用作注释的）。如果多行字符串被赋值给变量或在代码中有实际用途，它就是字符串值的一部分。如果多行字符串出现在代码中但<strong>未被赋值或使用</strong>，Python 解释器会忽略它，因此它可以“间接”用作多行注释。如果 <code>&#39;&#39;&#39;</code> 包裹的内容被赋值给变量或传递给函数，它是实际的字符串数据。</p>
<blockquote>
<p>当内容跨越多行时，使用 <code>&#39;&#39;&#39;</code> 可以保留原来的字符格式（包括换行），增强可读性。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 未赋值时（作为多行注释）</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">这是一个多行注释。</span><br><span class="line">它不会被执行，但可以在代码中保留说明。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">print(&quot;Hello, World!&quot;)</span><br><span class="line"></span><br><span class="line"># 作为多行字符串（赋值或使用时）</span><br><span class="line">text = &#x27;&#x27;&#x27;</span><br><span class="line">这是一个多行字符串。</span><br><span class="line">它可以包含换行和任意文本。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">print(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对比单行字符串，需要用 \n 手动添加换行符，写起来繁琐且不易维护</span><br><span class="line">prompt = &quot;You are a helpful assistant.\nAnswer the following question:\n&#123;question&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 使用多行字符串，格式清晰，换行自然保留。</span><br><span class="line">prompt = &#x27;&#x27;&#x27;</span><br><span class="line">You are a helpful assistant.</span><br><span class="line">Answer the following question:</span><br><span class="line">&#123;question&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="JSON格式与-Python-字典的区别"><a href="#JSON格式与-Python-字典的区别" class="headerlink" title="JSON格式与 Python 字典的区别"></a>JSON格式与 Python 字典的区别</h2><p><strong>本质</strong>：</p>
<ul>
<li>JSON 是<strong>数据交换格式</strong>，基于文本，跨语言支持；用于数据传输&#x2F;存储（如 API、文件）</li>
<li>Python 字典是<strong>内存中的数据结构</strong>，仅用于 Python；用于程序内部操作</li>
</ul>
<p><strong>语法差异</strong>：</p>
<ul>
<li>JSON 的键必须是<strong>字符串</strong>，且用<strong>双引号</strong> (“key”)。</li>
<li>Python 字典键可以是任何<strong>可哈希对象</strong>（如字符串、数字、元组）。</li>
<li>JSON 值仅支持：字符串、数字、布尔、null、对象、数组。</li>
<li>Python 字典值支持任意 Python 对象。</li>
<li>JSON 使用 true, false, null；Python 用 True, False, None。</li>
</ul>
<p><strong>序列化&#x2F;反序列化</strong>：</p>
<ul>
<li>JSON 需要通过 json.dumps()（序列化）或 json.loads()（反序列化）与 Python 字典互转</li>
<li>Python 字典可直接操作</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># JSON示例</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Alice&quot;,</span><br><span class="line">  &quot;age&quot;: 25,</span><br><span class="line">  &quot;is_student&quot;: true,</span><br><span class="line">  &quot;grades&quot;: [90, 85],</span><br><span class="line">  &quot;address&quot;: &#123;           </span><br><span class="line">    &quot;city&quot;: &quot;Beijing&quot;,</span><br><span class="line">    &quot;zip&quot;: &quot;100000&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;phone&quot;: null</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 字典示例</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Alice&quot;,  # 键是字符串</span><br><span class="line">  42: &quot;answer&quot;,     # 键是数字（JSON 不支持）</span><br><span class="line">  &quot;is_student&quot;: True,</span><br><span class="line">  &quot;grades&quot;: [90, 85],</span><br><span class="line">  &quot;address&quot;: &#123;</span><br><span class="line">    &quot;city&quot;: &quot;Beijing&quot;,</span><br><span class="line">    &quot;zip&quot;: &quot;100000&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;phone&quot;: None,</span><br><span class="line">  &quot;custom&quot;: lambda x: x  # 函数（JSON 不支持）</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>&#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25, &quot;active&quot;: true&#125;</code> 在特定情况下既可以看作 JSON，也可以看作 Python 字典</p>
<p><strong>作为 JSON</strong>：</p>
<ul>
<li>它是合法的 JSON 文本格式，符合 JSON 规范（键是双引号字符串，值是字符串&#x2F;数字&#x2F;布尔）。</li>
<li>通常用于数据传输或存储（例如 API 响应、文件）。</li>
<li>需要通过 json.loads() 解析成 Python 字典才能在 Python 中操作。</li>
</ul>
<p><strong>作为 Python 字典</strong>：</p>
<ul>
<li>它在 Python 中可以直接作为字典字面量运行（{“name”: “Alice”, “age”: 25, “active”: True}），因为语法兼容。</li>
<li>但注意，JSON 的 true 在 Python 中需解释为 True（Python 不认 true）。</li>
<li>如果直接写在 Python 代码中，它就是内存中的字典对象，无需解析。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line"># 作为 JSON 字符串</span><br><span class="line">json_str = &#x27;&#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25, &quot;active&quot;: true&#125;&#x27;</span><br><span class="line">py_dict = json.loads(json_str)  # 解析为 Python 字典</span><br><span class="line">print(py_dict)  # &#123;&#x27;name&#x27;: &#x27;Alice&#x27;, &#x27;age&#x27;: 25, &#x27;active&#x27;: True&#125;</span><br><span class="line"></span><br><span class="line"># 作为 Python 字典直接定义</span><br><span class="line">py_dict = &#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25, &quot;active&quot;: True&#125;</span><br><span class="line">print(py_dict)  # &#123;&#x27;name&#x27;: &#x27;Alice&#x27;, &#x27;age&#x27;: 25, &#x27;active&#x27;: True&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>JSON 用 <code>[]</code> 包住表示数组，通常用于表示多条数据；不带 <code>[]</code> 的是单个JSON对象。<code>[]</code> 常用于数据传输多涉及列表或集合。</p>
</blockquote>
<p>有时候会发现json数据前面被<code>[]</code>包住，表示它是一个 JSON 数组，而非单个 JSON 对象。JSON 的顶层结构可以是对象 <code>&#123;&#125;</code> 或数组 <code>[]</code>，具体取决于数据设计；<code>[]</code> 用于存储一组有序的数据（列表），通常表示多个记录或条目。例如，API 返回多条数据时，常以数组形式组织。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 表示多个对象（例如多条用户信息），对应 Python 的 list 包含多个 dict。</span><br><span class="line">[</span><br><span class="line">  &#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25&#125;,</span><br><span class="line">  &#123;&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 30&#125;,</span><br><span class="line">  &#123;&quot;name&quot;: &quot;Charlie&quot;, &quot;age&quot;: 35&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># JSON 对象（不带 []）</span><br><span class="line">&#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25&#125;</span><br></pre></td></tr></table></figure>

<h1 id="LangChain中的LLM和ChatModel有什么不同"><a href="#LangChain中的LLM和ChatModel有什么不同" class="headerlink" title="LangChain中的LLM和ChatModel有什么不同"></a>LangChain中的LLM和ChatModel有什么不同</h1><p>在 LangChain 框架中，<code>LLM</code>（Large Language Model）和 <code>ChatModel</code> 都是用于与大型语言模型交互的抽象接口，但它们的设计目的和输入&#x2F;输出格式有着本质的区别；<code>LLM</code> 和 <code>ChatModel</code> 是 <code>LangChain</code> 中实现调用方式的具体类</p>
<p>“补全型”和“对话型”描述的是<strong>模型交互的方式</strong>，是更抽象的概念，适用于任何支持这些交互的模型或框架。</p>
<p>LLM 和 ChatModel 是 LangChain 中的<strong>具体实现类</strong>，专为 LangChain 生态设计，封装了补全型和对话型的调用逻辑。</p>
<hr>
<p><strong>输入&#x2F;输出格式不同:</strong></p>
<ul>
<li><code>LLM</code>:主要处理字<strong>符串的</strong>输入和输出。给它一个文本字符串作为提示词（prompt），它返回一个文本字符串作为补全或生成的结果。更像是一个“文本补全”或“文本生成”引擎（补全型），使用单一 Prompt，适合单次文本生成。。<ul>
<li>输入: 单个字符串 (<code>&quot;请写一篇关于人工智能的短文。&quot;</code>)</li>
<li>输出: 单个字符串 (<code>&quot;人工智能是...&quot;</code>)</li>
</ul>
</li>
<li><code>ChatModel</code>:主要处理<strong>消息对象列表</strong>作为输入，并返回一个消息对象作为输出。它理解“对话轮次”和“角色”（如用户消息、AI 消息、系统消息），更适合进行多轮对话（对话型），使用消息列表（SystemMessage、HumanMessage 等），适合多轮对话和角色明确的任务。<ul>
<li>输入: 消息对象列表 (<code>[SystemMessage(content=&quot;你是一个乐于助人的助手。&quot;), HumanMessage(content=&quot;你好！&quot;)]</code>)</li>
<li>输出: 消息对象 (<code>AIMessage(content=&quot;你好！有什么可以帮助你的吗？&quot;)</code>)</li>
</ul>
</li>
</ul>
<p>**<code>LLM</code>:**更适合单轮的文本生成、提取、翻译、摘要等任务，其中输入是一个独立的文本块，不需要考虑之前的对话历史。如根据一段文本生成标题，或者对一个问题进行一次性回答。</p>
<p><strong><code>ChatModel</code>:</strong> 专门为多轮对话和构建聊天机器人而设计。它能够自然地处理对话历史，理解不同角色的发言，从而生成更连贯、上下文相关的回复。构建需要记忆和上下文的复杂 Agent 时，通常会使用 <code>ChatModel</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 使用LLM方式</span><br><span class="line">from langchain_community.llms import OpenAI # 示例</span><br><span class="line"></span><br><span class="line">llm = OpenAI() </span><br><span class="line">response = llm.invoke(&quot;请告诉我关于法国首都的信息。&quot;)</span><br><span class="line">print(response) # 输出字符串</span><br><span class="line"></span><br><span class="line"># 使用ChatModel</span><br><span class="line">from langchain_core.messages import HumanMessage, SystemMessage</span><br><span class="line">from langchain_openai import ChatOpenAI # 示例</span><br><span class="line"></span><br><span class="line">chat_model = ChatOpenAI()</span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=&quot;你是一个乐于助人的地理助手。&quot;),</span><br><span class="line">    HumanMessage(content=&quot;请告诉我关于法国首都的信息。&quot;)</span><br><span class="line">]</span><br><span class="line">response = chat_model.invoke(messages)</span><br><span class="line">print(response) # 输出 AIMessage 对象</span><br><span class="line">print(response.content) # 获取字符串内容</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left"><code>LLM</code></th>
<th align="left"><code>ChatModel</code></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>输入类型</strong></td>
<td align="left">字符串 (String)</td>
<td align="left">消息对象列表 (List of Message)</td>
</tr>
<tr>
<td align="left"><strong>输出类型</strong></td>
<td align="left">字符串 (String)</td>
<td align="left">消息对象 (Message)</td>
</tr>
<tr>
<td align="left"><strong>核心交互</strong></td>
<td align="left">文本补全 &#x2F; 单轮生成</td>
<td align="left">基于消息的多轮对话</td>
</tr>
<tr>
<td align="left"><strong>适用场景</strong></td>
<td align="left">单轮任务、文本生成、提取、摘要</td>
<td align="left">聊天机器人、Agent、多轮对话</td>
</tr>
<tr>
<td align="left"><strong>记忆&#x2F;上下文</strong></td>
<td align="left">不内置处理对话历史</td>
<td align="left">内置处理对话轮次和角色</td>
</tr>
</tbody></table>
<h1 id="LangChain中补全型（Completion）与对话型（Chat）的区别"><a href="#LangChain中补全型（Completion）与对话型（Chat）的区别" class="headerlink" title="LangChain中补全型（Completion）与对话型（Chat）的区别"></a>LangChain中补全型（Completion）与对话型（Chat）的区别</h1><blockquote>
<p>类似于上面的<code>LLM</code>与<code>ChatModel</code>的区别. 补全型 ≈ LLM 类；对话型 ≈ ChatModel 类；</p>
</blockquote>
<h4 id="补全型"><a href="#补全型" class="headerlink" title="补全型"></a>补全型</h4><ul>
<li><p>补全型指的是向语言模型提供一段文本（称为“提示”或 Prompt），模型根据这段文本生成后续的文本，完成内容的“补全”。<strong>不支持多轮对话，单次交互</strong></p>
</li>
<li><p>这种方式通常是单次交互，模型不维护对话上下文，输入和输出是一次性的。</p>
</li>
<li><p>没有明确的角色（如用户、助手），输入和输出是“连续的文本”。</p>
</li>
<li><p>适合场景：文本生成、文章续写、代码补全等。</p>
</li>
</ul>
<h4 id="对话型"><a href="#对话型" class="headerlink" title="对话型"></a>对话型</h4><ul>
<li><p>对话型（Chat）指的是以对话的形式与模型交互，输入是一系列明确角色的消息（比如 System、User、Assistant），模型根据这些消息生成回复。</p>
</li>
<li><p>这种方式通常支持多轮对话，模型可以理解上下文，适合模拟人机交互。</p>
</li>
<li><p>输入：由多个消息组成，每个消息有明确的角色（例如 System 设置背景，User 提出问题，Assistant 回复）。</p>
</li>
<li><p>输出：模型以对话形式返回回复，通常是 Assistant 角色的消息。</p>
</li>
<li><p>支持上下文管理，适合多轮对话。<strong>支持多轮对话，维护上下文</strong></p>
</li>
<li><p>适合场景：问答、聊天机器人、任务助手等。</p>
</li>
</ul>
<hr>
<p>LangChain 封装了多种语言模型，支持与补全型和对话型模型的交互，通过不同的接口和类来处理这两种调用方式，在下面的代码中体现为提示词的<code>prompt</code>与<code>messages</code>的区别；在导包时分别导入的是<code>OpenAI</code>与<code>ChatOpenAI</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 补全型调用</span><br><span class="line">from langchain_openai import OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model=&quot;text-davinci-003&quot;)  # 补全型模型</span><br><span class="line">prompt = &quot;Python 的列表推导式是什么？&quot;</span><br><span class="line">response = llm.invoke(prompt)</span><br><span class="line">print(response)</span><br><span class="line"></span><br><span class="line"># 对话型调用</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_core.messages import HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(model=&quot;gpt-4&quot;)</span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=&quot;你是一个编程助手。&quot;),</span><br><span class="line">    HumanMessage(content=&quot;Python 的列表推导式是什么？&quot;)</span><br><span class="line">]</span><br><span class="line">response = llm.invoke(messages)</span><br><span class="line">print(response.content)</span><br></pre></td></tr></table></figure>

<h1 id="Langchain-消息类型：角色与消息类的区别与用法"><a href="#Langchain-消息类型：角色与消息类的区别与用法" class="headerlink" title="Langchain 消息类型：角色与消息类的区别与用法"></a>Langchain 消息类型：角色与消息类的区别与用法</h1><p><code>&#39;user&#39;</code>, <code>&#39;assistant&#39;</code>, <code>&#39;system&#39;</code>, <code>&#39;tool&#39;</code> 是消息的<code>role</code>。</p>
<p><code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>, <code>ToolMessage</code> 是 Langchain 中用来表示这些<code>role</code>的 <strong>Python 类</strong>。两种方式都能实现工具调用，但消息类形式更直观。</p>
<p><strong>本质：</strong></p>
<ul>
<li>消息是结构化的数据，可以通过<strong>字典</strong>（包含 role 和 content）或<strong>消息类</strong>（如 SystemMessage、AIMessage）来表示。</li>
</ul>
<p><strong>两种写法的不同：</strong></p>
<ul>
<li><p><strong>字典形式</strong>：直接用 {‘role’: ‘user’, ‘content’: ‘文本’}的方式相对更加灵活，常用于直接与底层LLM模型的 API 进行交互或动态构造消息；适合低层次操作、调试，或直接传递给不支持 LangChain 消息类的模型。<strong>更接近底层 API，最好不在 Langchain 代码中常用</strong></p>
</li>
<li><p><strong>消息类形式</strong>：如 SystemMessage(content&#x3D;’文本’)、AIMessage(content&#x3D;’文本’)是 LangChain 的高级封装，更结构化，更加适合 LangChain 的生态。<strong>尽量在 LangChain 框架内使用消息类的形式</strong>（ 链、工具调用、聊天历史管理、ChatPromptTemplate、LCEL等），因为 LangChain 的组件更适配消息类。</p>
</li>
</ul>
<blockquote>
<p>为什么两种形式都存在，且在langchain中两种形式都可以使用？</p>
</blockquote>
<p>字典形式是模型底层的通用格式（例如 OpenAI 的 API 接受 role 和 content 的字典列表）。</p>
<p>消息类是 LangChain 为了简化开发、增强类型安全和功能（如 tool_calls、response_metadata）提供的封装。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 字典形式（使用 role 和 content）</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line"></span><br><span class="line"># 初始化模型</span><br><span class="line">model = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0)</span><br><span class="line"></span><br><span class="line"># 构造消息列表（字典形式）</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一个数学助手，只能回答数学相关问题。&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;计算 2 + 2&quot;&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># 调用模型</span><br><span class="line">response = model.invoke(messages)</span><br><span class="line"></span><br><span class="line"># 输出结果</span><br><span class="line">print(response.content)  # 输出：4</span><br><span class="line"></span><br><span class="line"># ------------------------------------------------------------------------------</span><br><span class="line"># ------------------------------------------------------------------------------</span><br><span class="line"># ------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># 消息类形式（使用 SystemMessage、HumanMessage ）</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_core.messages import SystemMessage, HumanMessage, AIMessage</span><br><span class="line"></span><br><span class="line"># 初始化模型</span><br><span class="line">model = ChatOpenAI(model=&quot;gpt-4o&quot;, temperature=0)</span><br><span class="line"></span><br><span class="line"># 构造消息列表（消息类形式）</span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=&quot;你是一个数学助手，只能回答数学相关问题。&quot;),</span><br><span class="line">    HumanMessage(content=&quot;计算 2 + 2&quot;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># 调用模型</span><br><span class="line">response = model.invoke(messages)</span><br><span class="line"></span><br><span class="line"># 输出结果</span><br><span class="line">print(response.content)  # 输出：4</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>角色 (<code>role</code>)</th>
<th>消息类</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>system</code></td>
<td><code>SystemMessage</code></td>
<td>系统指令，定义模型行为</td>
</tr>
<tr>
<td><code>user</code></td>
<td><code>HumanMessage</code></td>
<td>用户输入</td>
</tr>
<tr>
<td><code>assistant</code></td>
<td><code>AIMessage</code></td>
<td>模型回复，可包含 <code>tool_calls</code></td>
</tr>
<tr>
<td><code>tool</code></td>
<td><code>ToolMessage</code></td>
<td>工具调用结果</td>
</tr>
</tbody></table>
<h1 id="文本分割器"><a href="#文本分割器" class="headerlink" title="文本分割器"></a>文本分割器</h1><p>从高层次来看，文本分割器的工作原理如下：</p>
<ol>
<li>将文本拆分成小的、语义上有意义的块（通常是句子）。</li>
<li>开始将这些小块组合成一个更大的块，直到达到某个大小（通过某个函数来衡量）。</li>
<li>一旦达到该大小，将该块作为独立的文本片段，然后开始创建一个新的文本块，并保持一些重叠（以保持块之间的上下文）。</li>
</ol>
<h1 id="模型参数不同导致的训练时长和显存占用问题"><a href="#模型参数不同导致的训练时长和显存占用问题" class="headerlink" title="模型参数不同导致的训练时长和显存占用问题"></a>模型参数不同导致的训练时长和显存占用问题</h1><blockquote>
<p>我使用llama-factory来微调一个模型，跟着教程来做的  所有步骤都一样，只有一项不同：我用的是7B的模型，他用的是0.5B的模型，都是Qwen2.5模型  但是他的训练很快就完成了 我的训练很慢很慢 ，是什么原因？所有参数设置和数据集都是一样的</p>
</blockquote>
<ul>
<li><strong>主要原因在于模型大小差异巨大。</strong><ol>
<li><strong>模型参数量是关键：</strong> 我使用的是一个 70 亿参数 (7B) 的模型，而教程使用的是一个 5 亿参数 (0.5B) 的模型。7B 模型比 0.5B 模型大了整整 <strong>14倍</strong></li>
<li><strong>训练计算量与参数量直接相关：</strong> 模型训练（尤其是前向传播和反向传播计算梯度）的核心过程是对模型的<strong>所有参数</strong>进行大量的数学运算。参数量越大，每次训练迭代（一步）需要进行的计算就越多。</li>
<li><strong>每一步都更慢：</strong> 即使使用完全相同的数据集和参数设置，由于 7B 模型每一步需要处理的参数量是 0.5B 模型的 14 倍，因此 7B 模型完成每一步训练所需的时间会比 0.5B 模型长得多。</li>
<li><strong>总时间累积：</strong> 训练总时间是每一步训练所需时间乘以总的训练步数。即使总步数一样（取决于数据集大小和 epoch 数），由于每一步都慢很多，累积起来的总训练时间就会变得非常非常长。</li>
</ol>
</li>
</ul>
<p>所以即使数据集和所有其他参数都相同，仅仅因为训练的模型从 0.5B 变成了 7B，训练速度变慢是完全正常的，也是预期的结果。训练一个 7B 模型所需的计算资源（时间和硬件要求）远高于训练一个 0.5B 模型。</p>
<blockquote>
<p>还有一个问题,就是我在设置batch_size&#x3D;30  截断长度&#x3D;230时,别人能正常运行，而且显存占用不高 ，我同样的配置和参数设置，却提示了爆显存，只是模型选择不同，为什么会这样？</p>
</blockquote>
<p>原因依然是 <strong>模型参数大小不同</strong></p>
<p>显卡内存在模型训练时主要用于存储以下几个关键部分：</p>
<ol>
<li><strong>模型参数 (Model Parameters):</strong> 这是模型本身的权重和偏置。7B 模型有 70 亿个参数，0.5B 模型有 5 亿个参数。存储这些参数本身就需要大量的显存。<strong>7B 模型仅仅加载到显存中，所占用的空间就比 0.5B 模型大很多倍</strong>。</li>
<li><strong>优化器状态 (Optimizer States):</strong> 大多数现代优化器（如 Adam, AdamW）会为模型的每一个参数存储额外的状态信息（例如动量、方差等）。这些状态信息占用的显存通常是模型参数本身的 1 到 4 倍。因此，<strong>7B 模型的优化器状态占用的显存也是 0.5B 模型的许多倍</strong>。</li>
<li><strong>中间激活值 (Intermediate Activations):</strong> 在前向传播计算时，模型每一层都会产生输出（称为激活值），这些激活值需要在反向传播时用于计算梯度。激活值的大小与 Batch Size、序列长度以及模型的<strong>内部维度</strong>有关。虽然 Batch Size 和序列长度相同，但 7B 模型通常有更多的层和&#x2F;或更大的隐藏层维度，所以 <strong>即使 Batch Size 和序列长度相同，7B 模型产生的中间激活值通常也会比 0.5B 模型更大</strong>。</li>
<li><strong>梯度 (Gradients):</strong> 在反向传播时计算出的梯度需要存储起来，用于更新模型参数。梯度的数量与模型参数数量一致，所以 <strong>7B 模型的梯度占用的显存也是 0.5B 模型的很多倍</strong>。</li>
</ol>
<p><strong>为什么 7B 模型会爆显存而 0.5B 不会？</strong></p>
<ul>
<li><strong>0.5B 模型：</strong> 模型参数和优化器状态占用的基础显存相对较小。Batch Size&#x3D;30, 截断长度&#x3D;230 产生的激活值和梯度占用的显存加在上面，总和还在GPU 的容量范围内。</li>
<li><strong>7B 模型：</strong> <strong>模型参数和优化器状态占用的基础显存就已经非常巨大了。</strong> 当再加入 Batch Size&#x3D;30 和截断长度&#x3D;230 产生的激活值和梯度所需的显存时，这些额外的显存是叠加在一个已经非常庞大的基础之上的。即使 Batch Size 和截断长度带来的额外显存需求（激活值、梯度）比例上看起来相似，<strong>但由于基础占用巨大，总和很容易就超过了 GPU 的总显存容量</strong>，从而导致了爆显存。</li>
</ul>
<p>所以，即使保持了相同的 Batch Size 和截断长度，仅仅因为模型本身大小（参数量、内部维度）巨大，它所需要的总显存（尤其是参数、优化器状态、激活值和梯度的总和）就远超小模型，从而导致了显存不足。</p>
<h1 id="Qwen打包部署（大模型转换为-GGUF-以及使-用-ollama-运行）"><a href="#Qwen打包部署（大模型转换为-GGUF-以及使-用-ollama-运行）" class="headerlink" title="Qwen打包部署（大模型转换为 GGUF 以及使 用 ollama 运行）"></a>Qwen打包部署（大模型转换为 GGUF 以及使 用 ollama 运行）</h1>

	<div class="row">
    <embed src="https://statics.liuhengfeng.xyz/PDF/Qwen%E6%89%93%E5%8C%85%E9%83%A8%E7%BD%B2%EF%BC%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BA%20GGUF%20%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%20ollama%20%E8%BF%90%E8%A1%8C%EF%BC%89.pdf" width="100%" height="550" type="application/pdf">
	</div>




<h1 id="RAG中完成向量匹配之后传入LLM的数据类型是什么？"><a href="#RAG中完成向量匹配之后传入LLM的数据类型是什么？" class="headerlink" title="RAG中完成向量匹配之后传入LLM的数据类型是什么？"></a>RAG中完成向量匹配之后传入LLM的数据类型是什么？</h1><p>一般的RAG流程会先把源文档使用<code>Embedding Model</code>处理成向量的形式然后存储到向量数据库中，然后当用户提出问题<code>query</code>时，会使用同一个E<code>mbedding Model</code>对问题也做一次向量化，然后把问题和向量数据库中的内容做相似度对比，找出<code>top_k</code>个相似片段，然后把问题和找到的相似片段都传给LLM做推理和分析，找出符合问题的答案。</p>
<p>需要注意的是，<strong>传入LLM的数据是经过分片的原始文本表示</strong>，而不是向量化后的高维数据。</p>
<blockquote>
<p>RAG流程如下</p>
</blockquote>
<p><strong>步骤 1：文档处理（Offline&#x2F;Indexing）</strong></p>
<ul>
<li>原始文档被切分成小的、有意义的文本片段（chunks）。</li>
<li>每个文本片段通过<strong>嵌入模型</strong>转换为向量。</li>
<li>这些文本片段的向量和它们的<strong>原始文本内容</strong>一起存储在向量数据库中。通常，向量数据库会存储向量，并关联一个指向原始文本（或直接存储原始文本）的ID或元数据。</li>
</ul>
<p><strong>步骤 2：用户查询（Online&#x2F;Inference）</strong></p>
<ul>
<li>用户提出问题。</li>
<li>用户的查询通过<strong>同样的嵌入模型</strong>转换为向量。</li>
<li>这个查询向量用于在向量数据库中进行相似度搜索，查找与查询最相似的<code>top_K</code>个文档向量。</li>
</ul>
<p><strong>步骤 3：检索并获取原始文本：</strong></p>
<ul>
<li>从向量数据库中检索到的不仅仅是向量，更重要的是与这些<code>top_K</code>向量关联的<strong>原始文本片段（content）</strong>。</li>
</ul>
<p><strong>步骤 4：构建LLM提示：</strong></p>
<ul>
<li>用户的原始问题和检索到的<code>top_K</code>个原始文本片段被组织成一个结构化的提示（prompt），然后发送给LLM。</li>
</ul>
<p><strong>步骤 5：LLM生成答案：</strong></p>
<ul>
<li>LLM接收这个包含原始问题和相关上下文的提示，然后基于这些信息生成最终的答案。</li>
</ul>
<h1 id="能不能直接使用LLM作为RAG项目的嵌入模型？"><a href="#能不能直接使用LLM作为RAG项目的嵌入模型？" class="headerlink" title="能不能直接使用LLM作为RAG项目的嵌入模型？"></a>能不能直接使用LLM作为RAG项目的嵌入模型？</h1><p><strong>硬要用的话，其实是可以用的；但是一般不会这么用</strong>。RAG项目通常会使用一个<strong>单独的嵌入模型</strong>（Embedding Model）来将文档和用户查询转换为向量，而不是直接用生成式大模型（LLM）来做这件事。</p>
<blockquote>
<p>嵌入模型的本质</p>
</blockquote>
<p>嵌入模型是一种将离散数据（如文本、图像）映射到连续向量空间的技术。通过高维向量表示（常见的为768维~3072维），模型可捕捉数据的语义信息，使得语义相似的文本在向量空间中距离更近。例如，“忘记密码”和“账号锁定”、“河流”和“瀑布”会被编码为相近的向量，从而支持语义检索而非仅关键词匹配。</p>
<blockquote>
<p>主要是效率和成本问题</p>
</blockquote>
<p><strong>计算效率</strong>：将文档转换为向量并存储在向量数据库中需要一个高效的嵌入过程。专门的嵌入模型通常比LLM更小、更快、更节省资源。如果用LLM来做嵌入，每次调用（无论是文档预处理还是用户查询）都会消耗大量的计算资源和时间，成本会非常高。</p>
<p><strong>存储效率</strong>：生成式LLM内部的表示可能维度非常高，而且不一定是最紧凑有效的语义表示，可能导致向量数据库的存储开销更大。</p>
<p><strong>实时性要求：</strong>当用户提问时，需要实时地将查询转换为向量，然后快速地在向量数据库中进行相似度搜索。如果使用生成式LLM来做这个，延迟会非常高，影响用户体验。</p>
<p><strong>模型的职责分离和优化：</strong></p>
<ul>
<li><strong>嵌入模型（Embedding Model）</strong>：这类模型是专门为了将文本映射到高维向量空间而训练的。它们的目标是让语义相似的文本在向量空间中距离更近。例如，Sentence-BERT、OpenAI的<code>text-embedding-ada-002</code>、各种开源的通用嵌入模型（如<code>bge-large-en</code>、<code>all-MiniLM-L6-v2</code>等）。它们通常是<strong>双编码器（Bi-Encoder）</strong>架构，即查询和文档分别独立编码，然后计算相似度。这种架构在检索效率上非常高。</li>
<li><strong>生成式大模型（Generative LLM）</strong>：像GPT-3&#x2F;4、Llama、Mistral这类模型，它们的主要任务是<strong>理解并生成文本</strong>。虽然它们内部也有将文本编码为向量的过程，但这些内部向量（通常是最后一个隐藏层的输出）是针对生成任务优化的，<strong>并不直接适合作为通用语义相似度匹配的嵌入</strong>。它们通常是<strong>交叉编码器（Cross-Encoder）</strong>或解码器架构，需要同时输入查询和文档来计算相关性分数，这在检索大规模文档时效率极低。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Note_of_Learning(CS&amp;AI)</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://liuhengfeng.xyz/posts/c47516f6.html">https://liuhengfeng.xyz/posts/c47516f6.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>LHF</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2025-05-10</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2025-05-10</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a></div><div class="post_share"><div class="social-share" data-image="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-10_16-40-48.webp" data-sites="twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/b41d8a4a.html" title="pip包管理器常用命令及参数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-12_16-43-44.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">pip包管理器常用命令及参数</div></div></a></div><div class="next-post pull-right"><a href="/posts/39ea8be.html" title="2025-05"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-05_21-33-28_compressed.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2025-05</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/2ca90abc.html" title="AI基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/bc18e18a-2e2d-4378-9c15-f1c678fc5ed1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-19</div><div class="title">AI基础知识</div></div></a></div><div><a href="/posts/1a411b1e.html" title="AutoDL服务器常用命令记录"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-18_14-41-41.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-18</div><div class="title">AutoDL服务器常用命令记录</div></div></a></div><div><a href="/posts/e61f9869.html" title="DeepSeek模型原理与应用"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-15_14-07-51.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-15</div><div class="title">DeepSeek模型原理与应用</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LangChain-v0-3-x%E4%B8%8Epython%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">LangChain v0.3.x与python版本问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">解决方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Python%E8%AF%AD%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">Python语法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%85%83%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">2.1.</span> <span class="toc-text">三元表达式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%B7%E8%B1%A1%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88Walrus-Operator%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">海象运算符（Walrus Operator）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%A1%8C%E5%AD%97%E7%AC%A6%E4%B8%B2-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.3.</span> <span class="toc-text">多行字符串&#39;&#39;&#39; 的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JSON%E6%A0%BC%E5%BC%8F%E4%B8%8E-Python-%E5%AD%97%E5%85%B8%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.4.</span> <span class="toc-text">JSON格式与 Python 字典的区别</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LangChain%E4%B8%AD%E7%9A%84LLM%E5%92%8CChatModel%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="toc-number">3.</span> <span class="toc-text">LangChain中的LLM和ChatModel有什么不同</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LangChain%E4%B8%AD%E8%A1%A5%E5%85%A8%E5%9E%8B%EF%BC%88Completion%EF%BC%89%E4%B8%8E%E5%AF%B9%E8%AF%9D%E5%9E%8B%EF%BC%88Chat%EF%BC%89%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.</span> <span class="toc-text">LangChain中补全型（Completion）与对话型（Chat）的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%A5%E5%85%A8%E5%9E%8B"><span class="toc-number">4.0.0.1.</span> <span class="toc-text">补全型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E5%9E%8B"><span class="toc-number">4.0.0.2.</span> <span class="toc-text">对话型</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Langchain-%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B%EF%BC%9A%E8%A7%92%E8%89%B2%E4%B8%8E%E6%B6%88%E6%81%AF%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E7%94%A8%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">Langchain 消息类型：角色与消息类的区别与用法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">6.</span> <span class="toc-text">文本分割器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%B8%8D%E5%90%8C%E5%AF%BC%E8%87%B4%E7%9A%84%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%92%8C%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E9%97%AE%E9%A2%98"><span class="toc-number">7.</span> <span class="toc-text">模型参数不同导致的训练时长和显存占用问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Qwen%E6%89%93%E5%8C%85%E9%83%A8%E7%BD%B2%EF%BC%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BA-GGUF-%E4%BB%A5%E5%8F%8A%E4%BD%BF-%E7%94%A8-ollama-%E8%BF%90%E8%A1%8C%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">Qwen打包部署（大模型转换为 GGUF 以及使 用 ollama 运行）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RAG%E4%B8%AD%E5%AE%8C%E6%88%90%E5%90%91%E9%87%8F%E5%8C%B9%E9%85%8D%E4%B9%8B%E5%90%8E%E4%BC%A0%E5%85%A5LLM%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">9.</span> <span class="toc-text">RAG中完成向量匹配之后传入LLM的数据类型是什么？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%BD%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8LLM%E4%BD%9C%E4%B8%BARAG%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">10.</span> <span class="toc-text">能不能直接使用LLM作为RAG项目的嵌入模型？</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-wrap" id="footer-wrap"> <div class="copyright">&copy;2024 - 2025 By LHF</div><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script src="https://statics.liuhengfeng.xyz/js/jquery-3.6.1.min.js"></script><script async src="https://unpkg.com/vue@2.6.14/dist/vue.min.js"></script><script async src="https://unpkg.com/element-ui@2.15.6/lib/index.js"></script><script async src="https://liu-hexo.oss-cn-guangzhou.aliyuncs.com/js/lunar.min.js"></script><script src="/js/calendar.min.js"></script><script async src="https://statics.liuhengfeng.xyz/js/common.min.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false#  open shake (抖動特效);
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('card-widget card-announcement')[2];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = 'f89c5a6b949a4345a0d22f0ad5b040ea';
  var gaud_map_key = 'a36227a8c2b6dcd6b90a98920a06ac89';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '108.230574,22.793414';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://devapi.qweather.com/v7/weather/now?key=f89c5a6b949a4345a0d22f0ad5b040ea"></script><script data-pjax src="https://liu-hexo.oss-cn-guangzhou.aliyuncs.com/js/electric_clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="博客主题为Butterfly" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-d021d6?style=flat&amp;logo=buefy&amp;color=purple" alt=""/></a><a class="github-badge" target="_blank" href="https://pages.github.com/" style="margin-inline:5px" data-title="本站项目托管于Github Pages" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github%20Pages-d021d6?style=flat&amp;logo=githubpages&amp;color=brown" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="GitHub万岁" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Thanks-Github-d021d6?style=flat&amp;logo=GitHub&amp;color=indigo" alt=""/></a><a class="github-badge" target="_blank" href="https://www.alibabacloud.com/zh/product/content-delivery-network?_p_lc=1" style="margin-inline:5px" data-title="网站资源使用阿里云CDN加速" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/CDN-%E9%98%BF%E9%87%8C%E4%BA%91CDN-d021d6?style=flat&amp;logo=speedtest&amp;color=blue" alt=""/></a><a class="github-badge" target="_blank" href="https://www.aliyun.com/benefit" style="margin-inline:5px" data-title="本站图床部署于阿里云OSS" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/OSS-%E9%98%BF%E9%87%8C%E4%BA%91OSS-d021d6?style=flat&amp;logo=alibabacloud&amp;color=orange" alt=""/></a><a class="github-badge" target="_blank" href="https://beian.miit.gov.cn" style="margin-inline:5px" data-title="网站备案号" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/ICP-%E6%A1%82ICP%E5%A4%872023012461%E5%8F%B7-d021d6?style=flat&amp;logo=brandfolder&amp;color=yellow" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=creativecommons" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/4433d2d6.html" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/day_news.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-11-06</span><a class="blog-slider__title" href="posts/4433d2d6.html" alt="">每天60秒读懂世界</a><div class="blog-slider__text">每日新闻早报</div><a class="blog-slider__button" href="posts/4433d2d6.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/c47516f6.html" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/PixPin_2025-05-10_16-40-48.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-05-10</span><a class="blog-slider__title" href="posts/c47516f6.html" alt="">Note_of_Learning(CS&amp;AI)</a><div class="blog-slider__text">一些笔记杂记，主要是AI方面的</div><a class="blog-slider__button" href="posts/c47516f6.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/e4ce9af4.html" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/4de89523-2218-4748-8b18-5b961e39906d.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-01-06</span><a class="blog-slider__title" href="posts/e4ce9af4.html" alt="">2025</a><div class="blog-slider__text">My Life in 2025</div><a class="blog-slider__button" href="posts/e4ce9af4.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/93c9aa62.html" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://statics.liuhengfeng.xyz/Hexo/20241105bg2.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-11-05</span><a class="blog-slider__title" href="posts/93c9aa62.html" alt="">2024</a><div class="blog-slider__text">My Life In 2024!</div><a class="blog-slider__button" href="posts/93c9aa62.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://statics.liuhengfeng.xyz/js/swiper.min.js"></script><script defer data-pjax src="https://statics.liuhengfeng.xyz/js/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInRight');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('footer-wrap');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__backInUp');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('show');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInDown');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('is-center');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-data');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-social-icons');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-info-btn');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__fadeInLeft');
    arr[i].setAttribute('data-wow-duration', '500ms');
    arr[i].setAttribute('data-wow-delay', '300ms');
    arr[i].setAttribute('data-wow-offset', '180');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><!-- hexo injector body_end end --></body></html>